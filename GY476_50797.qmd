---
title: "Airbnb Geographic Analysis"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    fig-width: 8
    fig-height: 8

execute:
  warning: false
  error: false
  message: false
editor_options: 
  chunk_output_type: console
---

# Introduction

# Data Collection 

## 1.1 Collecting and importing the data

```{r libraries}
library(tidyverse)
library(sf)
library(tidylog)
library(mapview)
library(tmap)
# For painless filepath pointing
library(here)
library(janitor)
library(tidycensus)
library(leaflet.extras2)
library(paletteer)
library(ggpubr)
library(patchwork)
library(grid)
library(geomtextpath)
library(ggh4x)
library(units)
library(nngeo)
# acs_vars = load_variables(year = 2020, dataset = "acs5")

```

```{r data_read_in}
#| cache: true

#----- Read in data -------------

# ---- Airbnb data

# Helper fxn to download csv.gz file from Inside Airbnb, unzip and read in
download_and_readin_inside_airbnb_data = function(url, city){
  
  # Set data filepath
  data_filepath = here(str_glue("data/raw-data/{city}.csv.gz"))
  
  # Download data directly from Inside Airbnb website
  dir.create(here("data/raw-data/"), showWarnings = FALSE)
  download.file(url, destfile = data_filepath)


  # Read in data
  read_csv(here(data_filepath)) %>% 
    st_as_sf(coords = c("longitude", "latitude")) %>% 
    # Since coords are lot/lng, this is EPSG 4326
    st_set_crs("EPSG:4326") %>% 
    # To get nice and easily workable column names
    janitor::clean_names() %>% 
    mutate(city = city)
}


# Download and read in Airbnb data
sfo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-francisco/2022-09-07/data/listings.csv.gz",
                            city = "sfo")
oakland_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/oakland/2022-09-18/data/listings.csv.gz",
                            city = "oakland")
san_mateo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-mateo-county/2022-09-19/data/listings.csv.gz",
                            city = "san_mateo")

# Bind all data into one long dataframe, which is safe bc we have added a "city" column for easy identificiation
airbnb_data = bind_rows(oakland_data, sfo_data, san_mateo_data)

# ---- Zip code data

# Download and read in zip code data
dir.create("data/raw-data/", showWarnings = FALSE)
# Get download URL for geojson by examining API request logs on the website:
# https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q

# Try downloading Berkeley zip codes. Sometimes we get a 500 error in which
# case we use backup local copy
fpath  = tryCatch(
  expr = {download.file("https://geodata.lib.berkeley.edu/download/file/ark28722-s7888q-geojson.json",
              destfile = here("data", "bay_area_zip_codes.geojson"))
    here("data","bay_area_zip_codes.geojson") 
    },
  warning = function(w) {
    here("data","bay_area_zipcodes.geojson") 
    },
  error = function(e) {
    here("data","bay_area_zipcodes.geojson") 
    }
)

zip_codes = st_read(fpath, quiet = TRUE) %>% 
  clean_names()

# ---- ACS and ZCTA data

# Use get_acs from tidycensus
# Census servers often go down and get_acs fails intermittenetly. So if that
# happens read in local copy as backup
acs_data_raw = tryCatch(
  expr = {
    suppressMessages(get_acs(
                       # Get zcta level variables
                       geography = "zcta",
                       # Got these variable names by perusing data.census.gov website
                       variables = c(median_rent = "B25031_001",
                                     median_rent_as_pct_of_income = "B25071_001",
                                     unemp_rate_16_plus = "S2301_C04_001",
                                     num_bach_higher = "B23006_023",
                                     num_25_64_for_bach_higher = "B23006_001"
                                     # median_rent_no_bedroom = "B25031_002",
                                     # median_rent_1_bedroom = "B25031_003",
                                     # median_rent_2_bedroom ="B25031_004",
                                     # median_rent_3_bedroom = "B25031_005"
                                     ),
                       year = 2020,
                       survey = "acs5",
                       output = "wide",
                       # Allows us to get ZCTA geometries as sf dataframes
                       geometry = TRUE,
                       quiet = TRUE
    ) %>%
  janitor::clean_names() %>%
  # Calculate pct bach or higher using num and denom
  mutate(pct_bach_higher_e = num_bach_higher_e/num_25_64_for_bach_higher_e,
         pct_bach_higher_m = moe_ratio(
           num = num_bach_higher_e,
           denom = num_25_64_for_bach_higher_e,
           moe_num = num_bach_higher_m, 
           moe_denom = num_25_64_for_bach_higher_m
         )) %>%
  rename(zcta = geoid))
  },
  error = function (e) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  },
  warning = function (w) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  }
  )


# We will use acs_data in appendix to see how boundaries
acs_data = acs_data_raw %>% 
  st_drop_geometry()


```


# Data Discussion 


## Preparing the data

I make use of 4 datasets in this project:

1) Airbnb data: This is a periodically published snapshot of AirBnB listings in various global cities produced by Murray Cox on his website [Inside Airbnb](http://insideairbnb.com/). We programmatically download the zipped csc files containing listing data for San Francisco, San Mateo and Oakland, which will be our study area. The data for these three cities were scraped on 2022-09-07, 2022-09-19, and 2022-09-18 respectively and contains variables such as price, number of beds/baths, and review information for each of the `r nrow(airbnb_data)` total listings.

2) Bay Area Zipcode data: These are shapefiles of the 187 zip codes which comprise the Bay Area and was downloaded programmatically from the [Berkeley Geodata Library](https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q). 

3) ACS Demographic Data: Using `library(tidycensus)` we download demographic information for each Zip Code Tabulation Area (ZCTA) in the Bay Area from the 2016-2020 5 year ACS. Th3 5 year survey contains average values from 5 years of data collection in order to procide robust and precise esimates with low standard errors. The tradeoff is that these data are not the most recent and should instead be interpreted as roughly the average demographic composition within 2016-2020. This timeframe does not align  with the Airbmb data which are scra 

Specifically we look at the Median Rent per bedrooom, and the percentage of adults with at least a bachelors degree. These variables give us a rough estimate about the housing costs and education levels within each Bay Area neighborhood. The percentage of adults with at least a bachelor's degree was calculated from Census Table

ZCTAs are generalized representations of zip codes, and in most cases are very close zip codes. Sometimes a zip code may not have a corresponding ZCTA. In our case of the Bay Area, there is one zip code in Mountain View which does not have a corresonding ZCTA which we remove from our analysis. 

3) OSM Amenity Data

## Data Cleaning

For the Airbnb data, we convert the `price` column from a character vector into a numeric vector by removing commas, and `$` signs from the column. This will allow us to treat this column as numeric when performing analyses later. After some exploratory analysis, we also decide to drop the top 0.5% most expensive Airbnb listings from this analysis as they are very high outliers. This corresponds to dropping 67 listings with prices greater than or equal to $2500 per night. Upon visual inspection see that most of these listing don't have any reviews and may not be real, justifying our decision.

First we perform a spatial join to see how many Airbnb listings are in each zip code. When performing our spatial join, we notice that there are 32 Airbnb listings which fall out of the bounds of the Bay Area Zip Codes and do not get joined. Some of these may be coding errors as they are just a few feet into the ocean, while other listings are more than a mile away from the nearest zip code. To correct for this, I perform a second spatial join but this time join each of the 32 problematic listings to the closest zip code. If the distance between the closest zip and the listing is less than 100 meters (12 listings) I assign the closest zip code to the listing. And if the distance is more than 100 meters (20 listings), I drop it from this analysis.


```{r cleaning_data}
#| cache = TRUE

# -----  Join data ---------

# Join zip code data to ACS zcta level data. zcta = roughly zipcode for almost all cases
zip_code_demographic_data = zip_codes %>% 
  select(zip, po_name, state) %>% 
  # 1 zip code is in zip_codes but not in ACS data. Upon further inspection,
  # this is a zip code in Mountain View close to the water and not in our 
  # analysis area (ie there are no airbnb listings in this zip). So we drop this 
  # one zip code with inner_join
  tidylog::inner_join(acs_data, by = c("zip" = "zcta"))


# -----  Clean data ---------

# Clean up airbnb data
airbnb_data_cleaned = airbnb_data %>% 
  # Convert price from chr to numeric by removing $ and , characters
  mutate(price = str_replace_all(price, "\\$","") %>% 
           str_replace_all( ",", "") %>% 
           as.numeric()) %>% 
  select(id, price, listing_url, name, city, neighbourhood, neighbourhood_cleansed, property_type, bedrooms, bathrooms,number_of_reviews, has_availability, number_of_reviews, reviews_per_month, everything())



# -----  Drop Outliers ---------

# Drop 0.5% of highest priced outlier airbnb listings. 
# Top 0.5% of listings is effectively 67 listings with prices >= $2500
# Many of these outliers don't have any reviews and suggest they can be safely
# removed. There are 4 listings with a listed price of 0, but 2 do have
# reviews so we decide to keep them in
top_outlier_listings = airbnb_data_cleaned %>% 
  slice_max(price, 
            prop = 0.005) 

airbnb_data_cleaned = airbnb_data_cleaned %>% 
  # This effectively removes top 0.5% of highest priced listings, which is 
  # 67/13415 listings. Many of these outliers don't have any reviews and suggest
  # they can be safely removed. There are 4 listings with a listed price of 0, 
  # but 2 do have reviews so we decide to keep them in
  filter(price < 2500)


# Make histogram of dropped outliers
outlier_dropped_listings_price_histogram = top_outlier_listings %>% 
  ggplot(aes(x = price)) +
  geom_histogram(fill = "forestgreen", color = "white") +
  labs(title = "Histogram of Prices for top 0.5% of listings",
       x = "Price ($)", y = "Count (n)")

outlier_dropped_listings_price_histogram


```


## Projections 
I will use `EPSG:7131`or the NAD83 State Plane projection for San Franscisco for all our datasets as this was one of two projected coordinate reference systems specifically created by the San Francisco City and County GIS department. It is a conformal coordinate system that preserves angles and is suitable for the entirety of the Bay Area. 


```{r projecting_data}
#| cache = TRUE

airbnb_data_projected = airbnb_data_cleaned %>% 
  st_transform("EPSG:7131")
  
zip_code_demographic_data = zip_code_demographic_data %>% 
  st_transform("EPSG:7131")

zip_codes = zip_codes %>% 
  st_transform("EPSG:7131")

acs_data_raw = acs_data_raw %>% 
  st_transform("EPSG:7131")

```



## Data Cleaning


```{r perform_spatial_join}
#| cache = TRUE

# ----- Append ZCTA information to listings -------

# Perform spatial join, which accurately joins most listings to a zcta
airbnb_data_joined = airbnb_data_projected %>% 
  # st_join(zip_code_demographic_data %>% select(zip, po_name), 
  #         left_join = TRUE) %>% 
  st_join(
    acs_data_raw %>% 
      select(zcta, median_rent_e, pct_bach_higher_e, unemp_rate_16_plus_e),
    left = TRUE
  )


# There are 24 listings which aren't joined to a zcta, lets instead match those 
# if they're within 500 meters of a zcta. This allows all the listings to be 
# assigned to a zcta succesfully
problematic_listings_matched_zctas = airbnb_data_joined %>% 
  filter(is.na(zcta)) %>% 
  # Drop zcta colummn so we can take it from the acs_data_raw df after join
  select(-zcta) %>% 
  st_join(acs_data_raw %>% 
            select(zcta),
          left = TRUE,
          # Use nearest neighbor join to return closest zcta to point
          nngeo::st_nn, 
          # Set max distance of 500 meters
          maxdist = 500, 
          k = 1,
          progress = FALSE
  )


# Remove unmatched listings from data, and append with newly matched listings
airbnb_data_corrected = airbnb_data_joined %>% 
  filter(!is.na(zcta)) %>% 
  bind_rows(problematic_listings_matched_zctas)



# ---- Define Study Area -------

# Get shapefiles of all ZCTAs with at least 1 airbnb
acs_zctas_with_airbnbs = acs_data_raw %>% 
  filter(zcta %in% unique(airbnb_data_corrected$zcta) ) 

# Define study area to be the bbox of the zctas with at least 1 airnb
study_area_box = acs_zctas_with_airbnbs %>%
  st_bbox() 

# Expanding bounding box by a little bit to provide more 
# context and to make map shape more square
xrange <- study_area_box$xmax - study_area_box$xmin # range of x values
yrange <- study_area_box$ymax - study_area_box$ymin # range of y values

study_area_box[1] <- study_area_box[1] - (0.25 * xrange) # xmin - left
study_area_box[2] <- study_area_box[2] - (0.05 * yrange) # ymin - bottom
study_area_box[3] <- study_area_box[3] + (0.15 * xrange) # xmax - right
study_area_box[4] <- study_area_box[4] + (0.05 * yrange) # ymax - top

# ---- Cropping ZCTA data to study area ------

# Limit ZCTA data to study area
acs_data_cropped = acs_data_raw %>% 
  st_crop(study_area_box)

# ---- Define final datasets -----
airbnb_points = airbnb_data_corrected
acs_zcta_polygons = acs_data_cropped
```

### Map 1.1: Airbnb Listings per Zipcode


```{r listings_by_zipcode_map}
#| fig-width: 7
#| fig-height: 7


# ----- Count airbnb listings by zcta ----------
airbnb_count_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  count(zcta, sort = TRUE) %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(n, zcta, geometry) %>% 
  st_as_sf()


# Create buckets for price for use in histogram
airbnb_count_by_zcta = airbnb_count_by_zcta %>% 
  mutate(count_buckets = case_when(
    n >=0 & n <75 ~ "0 - 75",
    n >=75 & n <150 ~ "75 - 150",
    n >=150 & n <225 ~ "150 - 225",
    n >=225 & n <300 ~ "225 - 300",
    n >=300 & n <375 ~ "300 - 375",
    n >=375 & n <450 ~ "375 - 450",
    n >=450 & n <525 ~ "450 - 525",
    n >=525 ~ "525 +"
  ),
  count_buckets =
    as_factor(count_buckets) %>%
    fct_relevel(c("0 - 75", 
                  "75 - 150",
                  "150 - 225",
                  "225 - 300",
                  "300 - 375",
                  "375 - 450",
                  "450 - 525",
                  "525 +"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_count_by_zcta %>% 
      st_drop_geometry() %>% 
      select(num_listings = n, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------


listing_bucket_counts = airbnb_count_by_zcta %>% 
  filter(!is.na(count_buckets)) %>%
  st_drop_geometry() %>% 
  count(count_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    count_buckets == "525 +"  ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

# Create histogram which will double as legend
num_listings_histogram_legend = listing_bucket_counts %>% 
  ggplot(aes(x = n, y = count_buckets, fill = count_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = -11,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.233, "in"),
        legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold", size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "# Listings",
       x = "Count of Zip Codes")



# ----- Create choropleth map ----------


# Make map
airbnb_listings_by_zips_plot = airbnb_count_by_zcta %>% 
  ggplot(aes(fill = count_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none") +
  theme(plot.margin = margin())




# ----- Paste together with patchwork --------

design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_listings_by_zips_plot + 
  num_listings_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```


### Map 1.2: Average Airbnb Prices in Bay Area Zipcodes

```{r listing_avg_price_by_zip_map}
#| fig-width: 7
#| fig-height: 7

# ----- Calculate avg price by zcta ----------

airbnb_avg_price_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  group_by(zcta) %>% 
  summarize(avg_price = mean(price)) %>% 
  ungroup() %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(avg_price, zcta, geometry) %>% 
  st_as_sf()

# Look at histogram for designing cut points
# airbnb_avg_price_by_zcta$avg_price %>% hist()

# Create buckets for price for use in histogram
airbnb_avg_price_by_zcta = airbnb_avg_price_by_zcta %>% 
  mutate(avg_price_buckets = case_when(
    avg_price >=0 & avg_price <75 ~ "$0 - $75",
    avg_price >=75 & avg_price <150 ~ "$75 - $150",
    avg_price >=150 & avg_price <225 ~ "$150 - $225",
    avg_price >=225 & avg_price <300 ~ "$225 - $300",
    avg_price >=300 & avg_price <375 ~ "$300 - $375",
    avg_price >=375 & avg_price <450 ~ "$375 - $450",
    avg_price >=450 & avg_price <700 ~ "$450 - $700",
  ),
  avg_price_buckets =
    as_factor(avg_price_buckets) %>%
    # fct_expand("$500 - $600") %>%
    fct_relevel(c("$0 - $75", 
                  "$75 - $150",
                  "$150 - $225",
                  "$225 - $300",
                  "$300 - $375",
                  "$375 - $450",
                  "$450 - $700"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_avg_price_by_zcta %>% 
      st_drop_geometry() %>% 
      select(avg_price, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------

avg_price_bucket_counts = airbnb_avg_price_by_zcta %>% 
  filter(!is.na(avg_price_buckets)) %>%
  st_drop_geometry() %>% 
  count(avg_price_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    avg_price_buckets == "$450 - $700" ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

avg_price_histogram_legend = avg_price_bucket_counts %>% 
  ggplot(aes(x = n, y = avg_price_buckets, fill = avg_price_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = -11,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.275, "in"),
        legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold", size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "Avg Listing Price",
       x = "Count of Zip Codes")



# ----- Create choropleth map ----------
  
# Make map
airbnb_avg_price_by_zips_plot = airbnb_avg_price_by_zcta %>% 
  ggplot(aes(fill = avg_price_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none")


# ----- Paste together with patchwork --------


design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_avg_price_by_zips_plot + 
  avg_price_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```

For the demographic Census variables, I choose to look at housing costs (median rent per bedroom) and education levels (percentage of adults with at least a bachelor's degree). 

### Map 2.1: Median Rent in Bay Area by ZCTA

```{r median_rent_map}
#| fig-width: 6
#| fig-height: 7.25

# Create indicator for whether a zcta is in study area or not
acs_zcta_polygons = acs_zcta_polygons %>% 
  mutate(in_study_area = case_when(
    !is.na(num_listings) ~ TRUE,
    is.na(num_listings) ~ FALSE,
    TRUE ~ NA
  ))



# acs_zcta_polygons_study_area_adjusted %>% mapview(zcol)
# 
# acs_zcta_polygons %>% filter(is.na(median_rent_e)) %>% mapview()

# ----- Create raw median rent sf ggplot -------
median_rent_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = median_rent_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_viridis_c(
                    labels = scales::dollar_format(),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      )
                    ) +
    # Hacky solution to force ggplot to include NA in legend scale, create 
    # blank color aesthetic for a tiny zcta, and then override aesthetics in 
    # guide to match NA colro values
    geom_sf(data= acs_zcta_polygons %>%
            filter(zcta == 94517),
            size = 0.1,
          aes(color = "NA")) +
    scale_color_manual(values = NA) +
    guides(colour=guide_legend("", 
                               override.aes=list(colour="grey70", fill = "grey70"),
                               label.position = "left",
                               order =2)) +
    geom_sf(data = acs_zcta_polygons %>% 
            filter(!in_study_area),
          fill = "grey90",
          color = "white") +
  labs(fill = "Median Rent") +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      # legend.box.just = "right",
      # legend.text.align = 0
      )

# median_rent_map

# ----- Generate density histogram for median rent -------

# Generate density matrix for our variable, and convert into df
median_rent_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(median_rent_e) %>% 
  na.omit() %>% 
  stats::density()

median_rent_density_df = tribble(~median_rent_e, ~density,
       median_rent_density$x, median_rent_density$y) %>% 
  unnest(cols = c(median_rent_e, density))


median_rent_density_line = median_rent_density_df %>% 
  ggplot(aes(x = median_rent_e,
             y = density,
             color = median_rent_e,
             # fill = median_rent_e
             )) +
  geom_line(lwd = 1.5, alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.1,
                hjust = 0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
  scale_color_viridis_c(
                  direction = 1,
                  na.value = "grey90"
                  ) +
  scale_x_continuous(expand = expansion(mult = c(0.065, 0.23))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )
# median_rent_density_line
# ----- Paste everything together with library(patchwork) -------

design <- "
11111111111111111444
11111111111111111444
11111111111111111633
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111555
11111111111111111555
"    
median_rent_map +
  guide_area() + 
  median_rent_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")


```


### Map 2.2: Percentage of 25+ population with Bachelors Degree or Higher in Bay Area
```{r pct_bach_map}
#| fig-width: 6
#| fig-height: 7.25


# ----- Create raw % Bachelors sf ggplot -------

bach_degree_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = pct_bach_higher_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_viridis_c(
                    labels = scales::percent_format(),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      )
                    ) +
    # Don't need this hack to get NA in legend bc there are no NA values for this 
    # ACS variable
    # geom_sf(data= acs_zcta_polygons %>% 
    #         filter(zcta == 94517),
    #         size = 0.1,
    #       aes(color = "NA")) +
    # scale_color_manual(values = NA) +
    # guides(colour=guide_legend("", 
    #                            override.aes=list(colour="grey70", fill = "grey70"),
    #                            label.position = "left",
    #                            order =2)) +
    geom_sf(data = acs_zcta_polygons %>% 
            filter(!in_study_area),
          fill = "grey90",
          color = "white") +
  labs(fill = str_wrap(" % Bach. " ))  +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      legend.box.just = "right",
      legend.text.align = 0
      # legend.title = element_blank()
      )

# ----- Generate density histogram for % bach -------

# Generate density matrix for our variable, and convert into df
pct_bach_higher_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(pct_bach_higher_e) %>% 
  na.omit() %>% 
  stats::density()

pct_bach_higher_density_df = tribble(~pct_bach_higher_e, ~density,
       pct_bach_higher_density$x, pct_bach_higher_density$y) %>% 
  unnest(cols = c(pct_bach_higher_e, density))


# Create density histogram for our variable using ggplot and geom_line
# We need to use geom_line so we can get colors to vary along the line to match
# the legend coloring.
pct_bach_higher_density_line = pct_bach_higher_density_df %>% 
  ggplot(aes(x = pct_bach_higher_e,
             y = density,
             color = pct_bach_higher_e,
             )) +
  geom_line(lwd = 1.5, alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
  scale_color_viridis_c(
                  # labels = scales::percent_format(),
                  direction = 1,
                  na.value = "grey90"
                  ) +
  # Fiddled with these numbers until the density plot aligned with legend
  scale_x_continuous(expand = expansion(mult = c(0.11, 0.19))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(), 
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )


# ----- Paste everything together with library(patchwork) -------

design <- "
111111111111111111444
111111111111111111444
111111111111111111233
111111111111111111233
111111111111111111233
111111111111111111233
111111111111111111233
111111111111111111555
111111111111111111555
"    
bach_degree_map +
  guide_area() + 
  pct_bach_higher_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")



```

# Analysis Methods Explained

# Spatial Analysis

# Conclusion


# Appendix

## Appendix A: Relationship between Zip Code and ZCTA boundraies

The below interactive shows the zip code (left side) and ZCTA (right side) boundaries are slightly different. ZCTA are

```{r}
#| cache = TRUE


# zip_codes_cropped = zip_codes %>% 
#   st_crop(study_area_box)
# 
# acs_data_raw_cropped = acs_data_raw %>% 
#   st_transform("EPSG:7131") %>% 
#   st_crop(study_area_box)
# 
# mapview(zip_codes_cropped, color = "blue") | mapview(acs_data_raw_cropped, color = "blue")

```


