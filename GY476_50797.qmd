---
title: "Analyzing Airbnbs and Bikshare in San Francisco"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 7
    toc: true
  docx:
    fig-width: 7
    fig-height: 7
prefer-html: true
execute:
  warning: false
  error: false
  message: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r, load_libs}
#| output: false
#| eval: true
#| echo: false

library(tidyverse)
library(sf)
library(tidylog)
library(osmdata)
library(mapview)
library(leaflet)
library(leaflet.extras2)
library(leaflet.extras)
library(tmap)
# For painless filepath pointing
library(here)
library(janitor)
library(tidycensus)
library(leaflet.extras2)
library(paletteer)
library(ggpubr)
library(RColorBrewer)
library(patchwork)
library(grid)
library(geomtextpath)
library(ggh4x)
library(units)
library(nngeo)
library(ggsn)
library(hereR)
library(osrm)
library(dotenv)
library(gt)

# Set HERE API keys
dotenv::load_dot_env()
set_key(Sys.getenv("here_api_key"))

# acs_vars = load_variables(year = 2020, dataset = "acs5")

```

```{r data_read_in}
#| output: false
#| eval: true
#| echo: false
#| cache: true


#----- Read in data ------------

# ---- Airbnb data

# Helper fxn to download csv.gz file from Inside Airbnb, unzip and read in
download_and_readin_inside_airbnb_data = function(url, city){
  
  # Set data filepath
  data_filepath = here(str_glue("data/raw-data/{city}.csv.gz"))
  
  # Download data directly from Inside Airbnb website
  dir.create(here("data/raw-data/"), showWarnings = FALSE)
  download.file(url, destfile = data_filepath)


  # Read in data
  read_csv(here(data_filepath)) %>% 
    st_as_sf(coords = c("longitude", "latitude")) %>% 
    # Since coords are lot/lng, this is EPSG 4326
    st_set_crs("EPSG:4326") %>% 
    # To get nice and easily workable column names
    janitor::clean_names() %>% 
    mutate(city = city)
}


# Download airbnb data, I got these URLs from looking at developer console on Inside Airbnb webpage
sfo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-francisco/2022-09-07/data/listings.csv.gz",
                            city = "sfo")
oakland_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/oakland/2022-09-18/data/listings.csv.gz",
                            city = "oakland")
san_mateo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-mateo-county/2022-09-19/data/listings.csv.gz",
                            city = "san_mateo")

# Bind all data into one long dataframe, which is safe bc I have added a "city" column for easy identification
airbnb_data = bind_rows(oakland_data, sfo_data, san_mateo_data)

# ---- ACS and ZCTA data

# Use get_acs from tidycensus
# Census servers often go down and get_acs fails intermittently. So if that
# happens read in local copy as backup
acs_data_raw = tryCatch(
  expr = {
    suppressMessages(get_acs(
                       # Get zcta level variables
                       geography = "zcta",
                       # Got these variable names by perusing data.census.gov website
                       variables = c(median_rent = "B25031_001",
                                     median_rent_as_pct_of_income = "B25071_001",
                                     unemp_rate_16_plus = "S2301_C04_001",
                                     num_bach_higher = "B23006_023",
                                     num_25_64_for_bach_higher = "B23006_001"
                                     # median_rent_no_bedroom = "B25031_002",
                                     # median_rent_1_bedroom = "B25031_003",
                                     # median_rent_2_bedroom ="B25031_004",
                                     # median_rent_3_bedroom = "B25031_005"
                                     ),
                       year = 2020,
                       survey = "acs5",
                       output = "wide",
                       # Allows us to get ZCTA geometries as sf dataframes
                       geometry = TRUE
    ) %>%
  janitor::clean_names() %>%
  # Calculate pct bach or higher using num and denom
  mutate(pct_bach_higher_e = num_bach_higher_e/num_25_64_for_bach_higher_e,
         pct_bach_higher_m = moe_ratio(
           num = num_bach_higher_e,
           denom = num_25_64_for_bach_higher_e,
           moe_num = num_bach_higher_m, 
           moe_denom = num_25_64_for_bach_higher_m
         )) %>%
  rename(zcta = geoid))
  },
  error = function (e) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  },
  warning = function (w) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  }
  )

# Drop geometry column for now
acs_data = acs_data_raw %>% 
  st_drop_geometry()

# --- OSM Data ----

# build an overpass query to get bike rental points in Bay Area
query <- opq(bbox = "San Francisco Bay Area", timeout = 300) |> 
  add_osm_feature(key = "amenity", value = "bicycle_rental")

bike_rental_locations_osm <- osmdata_sf(query)

# Just extract locations which are encoded as points
bike_rental_points = bike_rental_locations_osm$osm_points %>% 
  as_tibble() %>% 
  st_as_sf() %>% 
  # These 2 columns seem to be the most useful for identifying bikes in network
  arrange(network, brand) %>% 
  select(osm_id, name, network, brand, everything())
  
# ---- Download SFO County boundary ------

# Download all CA counties
sfo_county = tigris::counties(state = "CA", 
                 cb = TRUE) %>% 
  clean_names %>% 
  # filter to San Francisco county
  filter(name == "San Francisco") 


# ---- Read in Bay Wheels bike data -----

# Choose to focus analysis on just one start station ID
chosen_station_id = "SF-F26"

# Helper function to download zip file and read in CSV
download_zip_and_read_csv = function(url){
  last_part_of_url = basename(url)
  download.file(url = url,
                quiet = TRUE, 
                destfile = str_glue("data/raw-data/{last_part_of_url}"))
  read_csv(str_glue("data/raw-data/{last_part_of_url}"))
  
}


bike_trip_data_nov = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202211-baywheeels-tripdata.csv.zip")
bike_trip_data_oct = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202210-baywheels-tripdata.csv.zip")
bike_trip_data_sep = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202209-baywheels-tripdata.csv.zip")
bike_trip_data_aug = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202208-baywheels-tripdata.csv.zip")
bike_trip_data_jul = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202207-baywheels-tripdata.csv.zip")
bike_trip_data_jun = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202206-baywheels-tripdata.csv.zip")

# Combine rows together, filter to just one start station
bike_trip_data_combined = bind_rows(
  bike_trip_data_jun,
  bike_trip_data_jul,
  bike_trip_data_aug, 
  bike_trip_data_sep,
  bike_trip_data_oct, 
  bike_trip_data_nov
  ) %>% 
  # filter to chosen station
  filter(start_station_id == chosen_station_id)
  

bike_trip_data = bike_trip_data_combined
```

#### GY476
#### Candidate 50797
#### Word Count: 2985

# Introduction

The last few decades have seen an explosion of new geographic data that describe the physical and digital world around us [@miller2010data]. In this paper, I explore spatial data within the San Francisco Bay Area, a region that is home to many companies responsible for the worlds geodata production. Part 1 will analyze how Airbnb listings and prices vary with housing costs and education levels and Part 2 will perform routing analysis to compare car and biking travel times between docks in the Bay Wheels bikeshare system in San Francisco. 

# Data
I make use of 4 datasets in this project:

-   *Airbnb listing data*: This is a dataset of Airbnb listings produced by [Inside Airbnb](http://insideairbnb.com/). I use the listing data for San Francisco, San Mateo and Oakland, which were scraped in September 2022. It contains variables like price and number of beds for each of the `r nrow(airbnb_data)` listings.

-   *ACS Demographic Data*: Using `library(tidycensus)` I download the median rent per bedroom, and the percentage of adults with at least a bachelors degree variables for each Zip Code Tabulation Area (ZCTA) in the Bay Area from the 2016-2020 5 year ACS. The percentage of adults with at least a bachelor's degree was calculated from Census Table B23006 and I use the number of adults over the age of 25 for the denominator.

-   *OSM Bike Rental data*: These are user reported point locations of bike rentals in Open Street Map and includes both private storefronts and bike rental systems/docking stations. Using `library(osmdata)`, I download Amenity data on the 253 bike rental stations in San Francisco.

-   *Bay Wheels Trip Data*: Bay Wheels is a subsidiary of Lyft and the regional bikeshare system serving the Bay Area. I use their public [system data](https://www.lyft.com/bikes/bay-wheels/system-data) from San Francisco over the last 6 months, which contains anonymized bike trip data including duration, time and dates, and start/end stations.

I programmatically download our data directly from download URLs wherever possible to make the data import process reproducible.

```{r data_read_in_code_show}
#| output: false
#| cache: false
#| eval: false
#| echo: true

library(tidyverse)
library(sf)
library(tidylog)
library(osmdata)
library(mapview)
library(leaflet)
library(leaflet.extras2)
library(leaflet.extras)
library(tmap)
# For painless filepath pointing
library(here)
library(janitor)
library(tidycensus)
library(leaflet.extras2)
library(paletteer)
library(ggpubr)
library(RColorBrewer)
library(patchwork)
library(grid)
library(geomtextpath)
library(ggh4x)
library(units)
library(nngeo)
library(ggsn)
library(hereR)
library(osrm)
library(dotenv)

# Set HERE API keys
dotenv::load_dot_env()
set_key(Sys.getenv("here_api_key"))



# ---- Read in Airbnb data ------

# Helper fxn to download csv.gz file from Inside Airbnb, unzip and read in
download_and_readin_inside_airbnb_data = function(url, city){
  
  # Set data filepath
  data_filepath = here(str_glue("data/raw-data/{city}.csv.gz"))
  
  # Download data directly from Inside Airbnb website
  dir.create(here("data/raw-data/"), showWarnings = FALSE)
  download.file(url, destfile = data_filepath)


  # Read in data
  read_csv(here(data_filepath)) %>% 
    st_as_sf(coords = c("longitude", "latitude")) %>% 
    # Since coords are lot/lng, this is EPSG 4326
    st_set_crs("EPSG:4326") %>% 
    # To get nice and easily workable column names
    janitor::clean_names() %>% 
    mutate(city = city)
}


# Download airbnb data, I got these URLs from looking at developer console on Inside Airbnb webpage
sfo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-francisco/2022-09-07/data/listings.csv.gz",
                            city = "sfo")
oakland_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/oakland/2022-09-18/data/listings.csv.gz",
                            city = "oakland")
san_mateo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-mateo-county/2022-09-19/data/listings.csv.gz",
                            city = "san_mateo")

# Bind all data into one long dataframe, which is safe bc I have added a "city" column for easy identification
airbnb_data = bind_rows(oakland_data, sfo_data, san_mateo_data)

# ---- Read in ACS and ZCTA data -----

# Use get_acs from tidycensus
# Census servers often go down and get_acs fails intermittently. So if that
# happens read in local copy as backup
acs_data_raw = tryCatch(
  expr = {
    suppressMessages(get_acs(
                       # Get zcta level variables
                       geography = "zcta",
                       # Got these variable names by perusing data.census.gov website
                       variables = c(median_rent = "B25031_001",
                                     median_rent_as_pct_of_income = "B25071_001",
                                     unemp_rate_16_plus = "S2301_C04_001",
                                     num_bach_higher = "B23006_023",
                                     num_25_64_for_bach_higher = "B23006_001"
                                     # median_rent_no_bedroom = "B25031_002",
                                     # median_rent_1_bedroom = "B25031_003",
                                     # median_rent_2_bedroom ="B25031_004",
                                     # median_rent_3_bedroom = "B25031_005"
                                     ),
                       year = 2020,
                       survey = "acs5",
                       output = "wide",
                       # Allows us to get ZCTA geometries as sf dataframes
                       geometry = TRUE
    ) %>%
  janitor::clean_names() %>%
  # Calculate pct bach or higher using num and denom
  mutate(pct_bach_higher_e = num_bach_higher_e/num_25_64_for_bach_higher_e,
         pct_bach_higher_m = moe_ratio(
           num = num_bach_higher_e,
           denom = num_25_64_for_bach_higher_e,
           moe_num = num_bach_higher_m, 
           moe_denom = num_25_64_for_bach_higher_m
         )) %>%
  rename(zcta = geoid))
  },
  error = function (e) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  },
  warning = function (w) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  }
  )

# Drop geometry column for now
acs_data = acs_data_raw %>% 
  st_drop_geometry()

# --- Read in OSM Data ----

# build an overpass query to get bike rental points in Bay Area
query <- opq(bbox = "San Francisco Bay Area", timeout = 90) |> 
  add_osm_feature(key = "amenity", value = "bicycle_rental")

bike_rental_locations_osm <- osmdata_sf(query)

# Just extract locations which are encoded as points
bike_rental_points = bike_rental_locations_osm$osm_points %>% 
  as_tibble() %>% 
  st_as_sf() %>% 
  # These 2 columns seem to be the most useful for identifying bikes in network
  arrange(network, brand) %>% 
  select(osm_id, name, network, brand, everything())
  
# ---- Download SFO County boundary ------

# Download all CA counties
sfo_county = tigris::counties(state = "CA", 
                 cb = TRUE) %>% 
  clean_names %>% 
  # filter to San Francisco county
  filter(name == "San Francisco") 


# ---- Read in Bay Wheels data -----

# Choose to focus analysis on just one start station ID
chosen_station_id = "SF-F26"

# Helper function to download zip file and read in CSV
download_zip_and_read_csv = function(url){
  last_part_of_url = basename(url)
  download.file(url = url,
                quiet = TRUE, 
                destfile = str_glue("data/raw-data/{last_part_of_url}"))
  read_csv(str_glue("data/raw-data/{last_part_of_url}"))
  
}


bike_trip_data_nov = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202211-baywheeels-tripdata.csv.zip")
bike_trip_data_oct = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202210-baywheels-tripdata.csv.zip")
bike_trip_data_sep = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202209-baywheels-tripdata.csv.zip")
bike_trip_data_aug = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202208-baywheels-tripdata.csv.zip")
bike_trip_data_jul = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202207-baywheels-tripdata.csv.zip")
bike_trip_data_jun = download_zip_and_read_csv(url = "https://s3.amazonaws.com/baywheels-data/202206-baywheels-tripdata.csv.zip")

# Combine rows together, filter to just one start station
bike_trip_data_combined = bind_rows(
  bike_trip_data_jun,
  bike_trip_data_jul,
  bike_trip_data_aug, 
  bike_trip_data_sep,
  bike_trip_data_oct, 
  bike_trip_data_nov
  ) %>% 
  # filter to chosen station
  filter(start_station_id == chosen_station_id)
  

bike_trip_data = bike_trip_data_combined
```

## Background on old and new spatial data

The ACS data we are using is a classic example of "old" spatial data, which are traditional, detailed datasets specifically designed for social science research. However, they are often very costly, published infrequently and only available at coarse resolutions.

This is in contrast to the Airbnb and Baywheels data, which are "new" forms of rich spatial data. These newer data are often collected automatically for business purposes and purely by accident are available for research [@arribas2014accidental]. These data can be very frequent (Airbnb data for example were all scraped within the last month), extremely granular, and are often much larger in size and scale [@miller2015data].

The disadvantages of new spatial data are that they can be biased [@o2017weapons], often do not measure uncertainty, and may have significant privacy concerns. In the case of Airbnb data, it only contains a small sliver of the rental market and its possible for fake listings to be included. In this paper I combine old and new forms of spatial data to understand how Airbnbs are distributed throughout the Bay and how travel times within San Francisco vary.

## Projections

I use `EPSG:7131`, or the NAD83 State Plane projection for San Francisco, in all our datasets as this was one of two projected coordinate reference systems specifically created by the San Francisco City and County GIS department. This is a projected 2D conformal coordinate system that:

-   preserves angles between locations
-   is accurate for distance based calculations like buffers and travel time
-   is suitable for the entirety of the Bay Area.

```{r projecting_data}
#| cache: true

airbnb_data = airbnb_data %>% 
  st_transform("EPSG:7131")

acs_data_raw = acs_data_raw %>% 
  st_transform("EPSG:7131")

bike_rental_points = bike_rental_points %>% 
  st_transform("EPSG:7131")

sfo_county = sfo_county %>% 
  st_transform("EPSG:7131")

```

## Data Cleaning

After exploratory analysis, I drop the top 0.5% most expensive Airbnb listings (67 listings with prices greater than \$2500 per night) as they are very high outliers. The vast majority of listings have prices between \$50 and \$500 a night. And upon visual inspection that most of the dropped listings don't have any reviews and may be fake, justifying our decision.

```{r cleaning_data}
#| cache: false
#| output: false

# -----  Clean Airbnb data ---------

# Clean up airbnb data
airbnb_data_cleaned = airbnb_data %>% 
  # Convert price from chr to numeric by removing $ and , characters
  mutate(price = str_replace_all(price, "\\$","") %>% 
           str_replace_all( ",", "") %>% 
           as.numeric()) %>% 
  select(id, price, listing_url, name, city, neighbourhood, neighbourhood_cleansed, property_type, bedrooms, bathrooms,number_of_reviews, has_availability, number_of_reviews, reviews_per_month, everything()) %>% 
  # If price = 0 (4 listings) convert to 1 so that taking log price returns sensible values
  mutate(
  price = case_when(
    price == 0 ~ 1,
    TRUE ~ price
    ),
  log_price = log(price),
  # Separate price into quartiles
  price_ntile = ntile(price, 4)
  )

# -----  Drop Airbnb Outliers ---------

# Drop 0.5% of highest priced outlier airbnb listings. 
# Top 0.5% of listings is effectively 67 listings with prices >= $2500
# Many of these outliers don't have any reviews and suggest they can be safely
# removed. There are 4 listings with a listed price of 0, but 2 do have
# reviews so I decide to keep them in
top_outlier_listings = airbnb_data_cleaned %>% 
  slice_max(price, 
            prop = 0.005) 

airbnb_data_cleaned = airbnb_data_cleaned %>% 
  # This effectively removes top 0.5% of highest priced listings, which is 
  # 67/13415 listings. Many of these outliers don't have any reviews and suggest
  # they can be safely removed. There are 4 listings with a listed price of 0, 
  # but 2 do have reviews so I decide to keep them in
  filter(price < 2500)


# Make histogram of dropped outliers
outlier_dropped_listings_price_histogram = top_outlier_listings %>% 
  ggplot(aes(x = price)) +
  geom_histogram(fill = "forestgreen", color = "white") +
  labs(title = "Histogram of Prices for top 0.5% of listings",
       x = "Price ($)", y = "Count (n)")

```

I filter the Bay Wheels bike data to trips that start at Station `SF-F26`, or Union Square. Routing analyses can blow up in size with many stations. And this is a relatively central station so results will be relatively transferable.

```{r clean_baywheels_bike_data}
#| cache: true
```

I also perform other data cleaning steps, including:

-   Removing small islands from the SF County boundary for easier to read maps
-   Removing probable duplicates from OSM bike rental locations
-   Taking the centroid of stations in the Bay Wheels data
-   Cropping all bike data to San Francisco
-   Assigning ZCTAs to each Airbnb with distance based joins
-   Creating a bounding box for our study area to crop our ACS data
-   Creating a hexagonal grid of our study area

More details can be found in the code below.

```{r data_cleaning}
#| cache: true
#| output: false

# ---- Clean Bike Share data -----

bike_trip_data_cleaned = bike_trip_data %>% 
  filter(!is.na(end_station_id)  & !is.na(start_station_id)) %>% 
  # One row with a seemingly invalid ID which I throw out
  filter(start_station_id != "Lab - Howard") %>%
  mutate(duration = ended_at - started_at) %>% 
  select(start_station_name,
         start_station_id,
         end_station_name,
         end_station_id,
         duration,
         everything()
         ) %>% 
  # Add id column which combines start and stations
  mutate(station_combo_id = str_glue("{start_station_id}_{end_station_id}"))


# Calculate all unique start and end stations from data
start_stations = bike_trip_data_cleaned %>% 
  distinct(start_station_id, start_lng, start_lat) %>% 
  st_as_sf(coords = c("start_lng", "start_lat"), crs = "EPSG:4326") %>%
  st_transform("EPSG:7131") %>% 
  group_by(start_station_id) %>% 
  # Same station id can have very slightly different lat/lons bc of differences
  # in bike racks. So I take centroid
  summarise(geometry = st_union(geometry), .groups = "keep") %>% 
  st_centroid()

end_stations = bike_trip_data_cleaned %>% 
  distinct(end_station_id, end_lng, end_lat) %>% 
  st_as_sf(coords = c("end_lng", "end_lat"), crs = "EPSG:4326") %>%
  st_transform("EPSG:7131") %>% 
  group_by(end_station_id) %>% 
  # Same station id can have very slightly different lat/lons bc of differences
  # in bike racks. So I take centroid
  summarise(geometry = st_union(geometry), .groups = "keep") %>% 
  st_centroid()



# ---- Clean up SFO county data ----

# Trim SFO boundary to just mainland SF, and remove small islands

# SF county includes some small islands like Treasure island, and an island in 
# the Pacific. To make our maps more readable, I remove these islands from the
# extent of the county. While removing the small islands is a small factual 
# omission, it will make the resulting maps much more readable.

sfo_county_boundary = sfo_county %>% 
  # Convert multiplication into polygon data, which splits up one row sf dataframe
  # into multi row sf dataframe with each distinct polygon getting its own row
  st_cast("POLYGON") %>% 
  mutate(area = st_area(.)) %>% 
  # Keep largest area polygon, which should just be mainland SF county polygon
  top_n(1, area)


# ---- Dedupe OSM bike data --------

# There are 169*2 stations that are within 3 meters of another station. These
# are probable duplicates so I remove these 169 probable duplicates
dupe_list = st_is_within_distance(bike_rental_points, bike_rental_points, dist = as_units(3, "m"))
duplicates_indices = unlist(map2(.x = dupe_list, 
                                 .y = seq_along(dupe_list), 
                                 .f = function(x,y) x[x < y]))
bike_rental_points_deduped = bike_rental_points[-duplicates_indices, ]



# Add flag for stations in the Bay Wheels or Ford GoBike networks
bike_rental_points_in_network = bike_rental_points_deduped %>% 
  mutate(is_in_baywheels = case_when(
    network == "Bay Wheels" | network == "Ford GoBike" | brand == "Bay Wheels" | brand =="Ford GoBike" ~ TRUE,
    TRUE ~ FALSE))



# # Calculate if bike stations are within 2 meters of each other
# bike_stations_within_2_m = bike_rental_points_in_network %>% 
#   st_is_within_distance(bike_rental_points_in_network %>% select(osm_id), 
#                         dist = 2)
# 
# # Helper function to take indexes from the sparse list returned above,
# # and create multipoint objects if multiple indices are returned
# convert_multiple_rows_to_one_multipoint_row = function(indexes){
#   # Take the indexes, slice the rows in the df, and union to make one
#   # multipoint object
#   bike_rental_points_in_network %>% 
#     slice(c(indexes)) %>% 
#     st_union() %>% 
#     st_as_sf() %>% 
#     rename(geometry = x)
#     # For multipoint geometries that are points within 2 meters of each other, 
#     # take the centroid of the 2 points for most accuracy, and to collapse to one point for future analysis
# }
# 
# bike_stations_deduplicated = map_df(bike_stations_within_2_m,
#     ~convert_multiple_rows_to_one_multipoint_row(.x %>% unlist))
# 
# 
# outlier_dropped_listings_price_histogram



# ----- Append ZCTA information to listings -------

# I append ZCTA information for each Airbnb by spatially joining the point 
# level Airbnb listings to the ZCTA polygons provided by the Census. This 
# spatial join allows us to successfully assign ZCTA for all but 24 of our 
# listings. Upon visual inspection, the remaining 24 listings seem to be
# just outside the ZCTA boundaries. In one case, the listing was a yacht that
# was a few hundred meters off the coast. In order to not lose valuable 
# information on these properties, I perform a second distance based spatial 
# join to see if the listings are within 500 meters of any ZCTAs which allows
# us to assign ZCTAs for all our listings. 

# Perform spatial join, which accurately joins most listings to a zcta
airbnb_data_joined = airbnb_data_cleaned %>% 
  st_join(
    acs_data_raw %>% 
      select(zcta, median_rent_e, pct_bach_higher_e, unemp_rate_16_plus_e),
    left = TRUE
  )


# There are 24 listings which aren't joined to a zcta, lets instead match those 
# if they're within 500 meters of a zcta. 
problematic_listings_matched_zctas = airbnb_data_joined %>% 
  filter(is.na(zcta)) %>% 
  # Drop zcta column so I can take it from the acs_data_raw df after join
  select(-zcta) %>% 
  st_join(acs_data_raw %>% 
            select(zcta),
          left = TRUE,
          # Use nearest neighbor join to return closest zcta to point
          join = nngeo::st_nn, 
          # Set max distance of 500 meters
          maxdist = 500, 
          k = 1,
          progress = FALSE
  )


# Remove unmatched listings from data, and append with newly matched listings
airbnb_data_corrected = airbnb_data_joined %>% 
  filter(!is.na(zcta)) %>% 
  bind_rows(problematic_listings_matched_zctas)


# ---- Define Study Area bbox -------

# The ZCTA level ACS data that `library(tidycensus)` produces is for the 
# entirety of the US. But will be making ZCTA level maps of the Bay Area and 
# therefore need to crop our data. I crop the ZCTA level ACS data to a study 
# area bounding box, which was created by:
# 
# 1) Calculating which ZCTAs our Airbnb listings fall into
# 2) Generating a bounding box around those ZCTAs
# 3) Expanding those bounding boxes by 1-5%
# 
# This gives us a rectangular region, which contains all the relevant ZCTAs with 
# Airbnb listings and a few other ZCTAs to help provide context on surrounding 
# neighborhoods and be used as a basemap layer. 


## Step 1) 
# Get geos of all ZCTAs with at least 1 airbnb
acs_zctas_with_airbnbs = acs_data_raw %>% 
  filter(zcta %in% unique(airbnb_data_corrected$zcta) ) 

## Step 2) 
# Define study area to be the bbox of the zctas with at least 1 airnb
study_area_box = acs_zctas_with_airbnbs %>%
  st_bbox() 

## Step 3) 
# Expanding bounding box by a little bit to provide more 
# context and to make map shape more square
xrange <- study_area_box$xmax - study_area_box$xmin # range of x values
yrange <- study_area_box$ymax - study_area_box$ymin # range of y values

study_area_box[1] <- study_area_box[1] - (0.25 * xrange) # xmin - left
study_area_box[2] <- study_area_box[2] - (0.05 * yrange) # ymin - bottom
study_area_box[3] <- study_area_box[3] + (0.15 * xrange) # xmax - right
study_area_box[4] <- study_area_box[4] + (0.05 * yrange) # ymax - top


# Limit ZCTA data to study area
acs_data_cropped = acs_data_raw %>% 
  st_crop(study_area_box)

# Define final datasets
airbnb_points = airbnb_data_corrected
acs_zcta_polygons = acs_data_cropped


# ---- Crop Bike data to SF ----
# Crop OSM bike rental points to SF
bike_rental_points_sfo = bike_rental_points_in_network %>% 
  # Rename name column bc LHS df also has name column
  st_join(sfo_county_boundary %>% select(county = name),
          left= FALSE)

# Crop BayWheels Bike data to SF
# Calculate all stations in sfo from end_station
sfo_end_stations = end_stations %>% 
  st_join(sfo_county_boundary %>% 
            select(county_name = name),
          left= FALSE)

# Limit to trips within SFO, which is all the trips
bike_trip_data_sfo = bike_trip_data_cleaned %>% 
  filter(end_station_id %in% sfo_end_stations$end_station_id) 



# ---- Create hex honeycomb grids ----

# Create blank hex grid
airbnb_area_honeycomb_grid = st_make_grid(airbnb_points, 
                                   c(1000, 1000), 
                                   what = "polygons", 
                                   square = FALSE) %>% 
                            st_as_sf() %>% 
  mutate(hex_id = row_number()) %>% 
  rename(geometry = x)

### Calculate hexagon level Airbnb vars ###

# Join airbnb point data to hex grid polygon data
airbnb_points = airbnb_points %>% 
  st_join(airbnb_area_honeycomb_grid,
          left = TRUE) 

# Calculate hex level data on # listings and avg price
hex_grid_listing_info = airbnb_points %>% 
  st_drop_geometry() %>% 
  group_by(hex_id) %>% 
  summarize(avg_hex_price = mean(price, na.rm = TRUE),
            avg_log_hex_price = mean(log_price, na.rm = TRUE),
            number_of_listings = n()) %>% 
  ungroup() %>% 
  mutate(avg_hex_price_ntile = as_factor(ntile(avg_hex_price, 6)),
         number_of_listings_ntile = as_factor(ntile(number_of_listings, 6))
         )

### Calculate hexagon level ACS vars ###

# Join zcta polygon data to hex grid polygon data
zcs_zcta_polygons_hex_polygons_ints = acs_zcta_polygons %>% 
  mutate(zcta_area = st_area(.)) %>% 
  st_intersection(
    airbnb_area_honeycomb_grid %>% 
      mutate(hex_area = st_area(.))  %>% 
      select(hex_id, hex_area),
    left = TRUE
  ) %>% 
  mutate(area_ints = st_area(.)) %>% 
  ungroup()

# Calculate hex level ACS values by taking area weighted average across zcta intersections
hex_grid_zcta_information = zcs_zcta_polygons_hex_polygons_ints %>% 
  st_drop_geometry() %>% 
  group_by(hex_id) %>% 
  # Calculate the pct of the hex area that falls into each zcta intersection
  mutate(pct_zcta_in_hex = (area_ints / sum(area_ints))) %>% 
  # Calculate area weighted average across zctas the hex falls into, and 
  # collapse into one row per hex
  summarize(pct_bach_higher_e = sum(pct_zcta_in_hex * pct_bach_higher_e),
         median_rent_e = sum(pct_zcta_in_hex * median_rent_e),
         hex_id = first(hex_id),
         sum_pct_zcta_in_hex = sum(pct_zcta_in_hex)
         )


### Calculate hexagon level bike vars ###

# Join bike point data to hex grid polygon data
x = bike_rental_points_sfo %>% 
  st_join(airbnb_area_honeycomb_grid,
          left = TRUE) 

# Calculate hex level data on # listings and avg price
hex_grid_bike_info = x %>% 
  st_drop_geometry() %>% 
  group_by(hex_id) %>% 
  summarize(num_stations = n())


### Get correct geos for hexes ###

# Left join with original hexagons for correct geometries
# this has 3382 hexagons as it covers the whole study area
hex_grid_acs_variables = airbnb_area_honeycomb_grid %>% 
  left_join(hex_grid_zcta_information,
            by = "hex_id")

hex_grid_airbnb_variables = airbnb_area_honeycomb_grid %>% 
  left_join(hex_grid_listing_info,
            by = "hex_id"
            )

hex_grid_bike_variables = airbnb_area_honeycomb_grid %>% 
  left_join(hex_grid_bike_info,
            by = "hex_id")
```

# Part 1: Mapping/Data Viz

## Mapping Airbnbs

We map the count (Map 1.1) and average price (Map 2.2) of Airbnb listings in the Bay Area. I made several design decisions which are justified below:

1)  I bin the continuous data into equal intervals because the ranges of our variables are relatively large and I want to avoid having too many colors. Using equal interval bins makes the resulting ranges easy to interpret.

2)  I include the number of zip codes that fall in each bin as a histogram to the right of the legend. This visualizes the actual distribution of the variable and provides context on what may seem like arbitrary bin cutoff decisions.

3)  I use the Green-Blue color palette from [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=GnBu&n=6), based on Cynthia Brewer's research [-@harrower2003colorbrewer] on gradients that are colorblind friendly, and suitable for LCD computer screens.

4)  I provide interactive maps using `library(mapview)` for exploratory purposes in the `interactive` tabs.


### Map 1.1: Count of Airbnb listings

::: panel-tabset
#### Static

```{r listings_by_zipcode_map}
#| fig-width: 7
#| fig-height: 7
#| cache: true

# ----- Count airbnb listings by zcta ----------
airbnb_count_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  count(zcta, sort = TRUE) %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(n, zcta, geometry) %>% 
  st_as_sf()


# Create buckets for price for use in histogram
airbnb_count_by_zcta = airbnb_count_by_zcta %>% 
  mutate(count_buckets = case_when(
    n >=0 & n <75 ~ "0 - 75",
    n >=75 & n <150 ~ "75 - 150",
    n >=150 & n <225 ~ "150 - 225",
    n >=225 & n <300 ~ "225 - 300",
    n >=300 & n <375 ~ "300 - 375",
    n >=375 & n <450 ~ "375 - 450",
    n >=450 ~ "450 +"
  ),
  count_buckets =
    as_factor(count_buckets) %>%
    fct_relevel(c("0 - 75", 
                  "75 - 150",
                  "150 - 225",
                  "225 - 300",
                  "300 - 375",
                  "375 - 450",
                  "450 +"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_count_by_zcta %>% 
      st_drop_geometry() %>% 
      select(num_listings = n, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------


listing_bucket_counts = airbnb_count_by_zcta %>% 
  filter(!is.na(count_buckets)) %>%
  st_drop_geometry() %>% 
  count(count_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    count_buckets == "450 +"  ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

# Create histogram which will double as legend
num_listings_histogram_legend = listing_bucket_counts %>% 
  ggplot(aes(x = n, y = count_buckets, fill = count_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.275, "in"),
        legend.key.width = unit(0.05, "in"),
#         legend.key.height = unit(0.233, "in"),
#         legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold.italic", 
                                 size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "# Listings",
       x = "Count of Zip Codes")


# ----- Create choropleth map ----------

# Try to create one scalebar for use in all our maps
themed_scalebar =   scalebar(airbnb_count_by_zcta, 
           dist = 5, 
           dist_unit = "km", 
           transform = FALSE,
           location = "bottomleft",
           height = 0.01,
           st.size = 4,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2)

# Make map
airbnb_listings_by_zips_plot = airbnb_count_by_zcta %>% 
  ggplot(aes(fill = count_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none") +
  theme(plot.margin = margin()) +
  themed_scalebar
  # scalebar(airbnb_count_by_zcta, 
  #          dist = 5, 
  #          dist_unit = "km", 
  #          transform = FALSE,
  #          location = "bottomleft",
  #          height = 0.01,
  #          st.size = 4,
  #          st.color = "grey80", 
  #          box.fill = c("gray90", "gray98"),
  #          box.color = "gray90", 
  #          border.size = 0.2)


# ----- Paste together with patchwork --------

design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_listings_by_zips_plot + 
  num_listings_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```

Most ZCTAs have 0-75 listings. The ZCTAs with the highest count of listings are concentrated in downtown San Francisco & right next to the Bay in West Oakland. ZCTAs with the least amount of Airbnbs tend to be in far east Oakland and Southwest Bay Area. This makes sense as these locations are far from common tourist destinations.

#### Interactive

```{r}
#| cache: false
mapviewOptions(basemaps = "CartoDB.Positron")
mapview(airbnb_count_by_zcta, 
        zcol = "count_buckets", 
        layer = "Count of Listings",
        col.regions=brewer.pal(7, "GnBu"))
```
:::

### Map 1.2: Average Airbnb Prices in Bay Area Zipcodes

::: panel-tabset
#### Static

```{r listing_avg_price_by_zip_map}
#| fig-width: 7
#| fig-height: 7
#| cache: true


# ----- Calculate avg price by zcta ----------

airbnb_avg_price_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  group_by(zcta) %>% 
  summarize(avg_price = mean(price)) %>% 
  ungroup() %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(avg_price, zcta, geometry) %>% 
  st_as_sf()

# Look at histogram for designing cut points
# airbnb_avg_price_by_zcta$avg_price %>% hist()

# Create buckets for price for use in histogram
airbnb_avg_price_by_zcta = airbnb_avg_price_by_zcta %>% 
  mutate(avg_price_buckets = case_when(
    avg_price >=0 & avg_price <75 ~ "$0 - $75",
    avg_price >=75 & avg_price <150 ~ "$75 - $150",
    avg_price >=150 & avg_price <225 ~ "$150 - $225",
    avg_price >=225 & avg_price <300 ~ "$225 - $300",
    avg_price >=300 & avg_price <375 ~ "$300 - $375",
    avg_price >=375 & avg_price <450 ~ "$375 - $450",
    avg_price >=450 & avg_price <700 ~ "$450 - $700",
  ),
  avg_price_buckets =
    as_factor(avg_price_buckets) %>%
    # fct_expand("$500 - $600") %>%
    fct_relevel(c("$0 - $75", 
                  "$75 - $150",
                  "$150 - $225",
                  "$225 - $300",
                  "$300 - $375",
                  "$375 - $450",
                  "$450 - $700"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_avg_price_by_zcta %>% 
      st_drop_geometry() %>% 
      select(avg_price, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------

avg_price_bucket_counts = airbnb_avg_price_by_zcta %>% 
  filter(!is.na(avg_price_buckets)) %>%
  st_drop_geometry() %>% 
  count(avg_price_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    avg_price_buckets == "$450 - $700" ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

avg_price_histogram_legend = avg_price_bucket_counts %>% 
  ggplot(aes(x = n, y = avg_price_buckets, fill = avg_price_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = -11,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.275, "in"),
        legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold.italic",
                                 size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "Avg Listing Price",
       x = "Count of Zip Codes")



# ----- Create choropleth map ----------
  
# Make map
airbnb_avg_price_by_zips_plot = airbnb_avg_price_by_zcta %>% 
  ggplot(aes(fill = avg_price_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none") +
  scalebar(airbnb_avg_price_by_zcta, 
           dist = 5, 
           dist_unit = "km", 
           transform = FALSE,
           location = "bottomleft",
           height = 0.01,
           st.size = 4,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2)

# ----- Paste together with patchwork --------


design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_avg_price_by_zips_plot + 
  avg_price_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```

Most Airbnbs tend to be between \$150 - \$300 a night. The highest priced Airbnbs are along the coastline of the Pacific Ocean and SF. These Airbnbs probably have great views, or access to beaches/other tourist amenities. Conversely, the lowest priced Airbnbs tend to be in Oakland.

#### Interactive

```{r}
#| cache: false

mapview(airbnb_avg_price_by_zcta, 
        zcol = "avg_price_buckets", 
        layer = "Avg Listing Price",
        col.regions=brewer.pal(7, "GnBu"))
```
:::

## Mapping Census variables

We map our two Census variables at ZCTA level. Our maps are similar to the previous maps but with a few different decisions:

1)  I use a continuous color ramp as we are interested in looking at small differences in the Census variables across ZCTAs.

2)  I include a color matched density plot to the right of the legend. This is essentially a smoothed version of a histogram. I choose to omit the axis for the density plot as actual density values can be hard to interpret (they are just meant to integrate to 1) and I want the reader to focus on the overall distribution of the variable.

### Map 2.1: [Median Rent]{style="color:#2171b5"} by ZCTA

::: panel-tabset
#### Static

```{r median_rent_map}
#| fig-width: 6
#| fig-height: 7.25
#| cache: false


# ----- Create raw median rent sf ggplot -------

# Create indicator for whether a zcta is in study area or not
acs_zcta_polygons = acs_zcta_polygons %>% 
  mutate(in_study_area = case_when(
    !is.na(num_listings) ~ TRUE,
    is.na(num_listings) ~ FALSE,
    TRUE ~ NA
  ))


median_rent_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = median_rent_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_distiller(
                    labels = scales::dollar_format(),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      ),
                    palette = "Blues"


  ) +
  # Hacky solution to force ggplot to include NA in legend scale, create 
  # blank color aesthetic for a tiny zcta, and then override aesthetics in 
  # guide to match NA color values
  geom_sf(data= acs_zcta_polygons %>%
          filter(zcta == 94517),
          size = 0.1,
        aes(color = "NA")) +
  scale_color_manual(values = NA) +
  guides(colour=guide_legend("", 
                             override.aes=list(colour="grey70", fill = "grey70"),
                             label.position = "left",
                             order =2)) +
  geom_sf(data = acs_zcta_polygons %>% 
          filter(!in_study_area),
        fill = "grey90",
        color = "white") +
  labs(fill = "Median Rent") +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      # legend.box.just = "right",
      # legend.text.align = 0
      ) +
   scalebar(acs_zcta_polygons, 
           dist = 5, 
           dist_unit = "km", 
           transform = FALSE,
           location = "bottomleft",
           height = 0.01,
           st.size = 4,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2)



# ----- Generate density histogram for median rent -------

# Generate density matrix for our variable, and convert into df
median_rent_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(median_rent_e) %>% 
  na.omit() %>% 
  stats::density()

median_rent_density_df = tribble(~median_rent_e, ~density,
       median_rent_density$x, median_rent_density$y) %>% 
  unnest(cols = c(median_rent_e, density))


median_rent_density_line = median_rent_density_df %>% 
  ggplot(aes(x = median_rent_e,
             y = density,
             color = median_rent_e,
             # fill = median_rent_e
             )) +
  geom_line(lwd = 1.5, alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.1,
                hjust = 0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
  scale_color_distiller(
                  direction = 1,
                  na.value = "grey90",
                  palette = "Blues"
  ) +
  # scale_color_viridis_c(
  #                 # labels = scales::percent_format(),
  #                 direction = 1,
  #                 na.value = "grey90"
  #                 ) +
  scale_x_continuous(expand = expansion(mult = c(0.065, 0.23))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )
# median_rent_density_line
# ----- Paste everything together with library(patchwork) -------

design <- "
11111111111111111444
11111111111111111444
11111111111111111633
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111555
11111111111111111555
"    
median_rent_map +
  guide_area() + 
  median_rent_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")


```

The ZCTAs with the highest rent tend to be in downtown San Francisco and Southeast San Mateo. The ZCTAs with the lowest rent tend to be in Southeast San Francisco, and all throughout Oakland. The density plot reveals that most ZCTAs have average rents between \$1500 and \$2500 per bedroom.

#### Interactive

```{r}
#| cache: false

acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  mapview(zcol = "median_rent_e",
          layer = "Median Rent",
          col.regions=brewer.pal(7, "Blues"),
          na.color = "grey40") +
  mapview(acs_zcta_polygons %>% filter(!in_study_area),
          col.regions = "grey80",
          legend = FALSE)
  
```
:::

### Map 2.2: Percentage of 25+ population with [Bacherlors Degree or higher]{style="color:#2171b5"}


::: panel-tabset
#### Static

```{r pct_bach_map}
#| fig-width: 6
#| fig-height: 7.25
#| cache: false


# ----- Create raw % Bachelors sf ggplot -------

bach_degree_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = pct_bach_higher_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_distiller(
                    labels = scales::percent_format(),
                    limits = c(0,1),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      ),
                    palette = "Blues"
                    
    
  ) +
  geom_sf(data = acs_zcta_polygons %>% 
            filter(!in_study_area),
          fill = "grey90",
          color = "white") +
  labs(fill = str_wrap(" % Bach. " ))  +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      legend.box.just = "right",
      legend.text.align = 0
      # legend.title = element_blank()
      ) +
   scalebar(acs_zcta_polygons, 
           dist = 5, 
           dist_unit = "km", 
           transform = FALSE,
           location = "bottomleft",
           height = 0.01,
           st.size = 4,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2)


# ----- Generate density histogram for % bach -------

# Generate density matrix for our variable, and convert into df
pct_bach_higher_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(pct_bach_higher_e) %>% 
  na.omit() %>% 
  stats::density()

pct_bach_higher_density_df = tribble(~pct_bach_higher_e, ~density,
       pct_bach_higher_density$x, pct_bach_higher_density$y) %>% 
  unnest(cols = c(pct_bach_higher_e, density))


# Create density histogram for our variable using ggplot and geom_line
# I need to use geom_line so I can get colors to vary along the line to match
# the legend coloring.
pct_bach_higher_density_line = pct_bach_higher_density_df %>% 
  ggplot(aes(x = pct_bach_higher_e,
             y = density,
             color = pct_bach_higher_e,
             )) +
  geom_line(lwd = 1.5, 
            alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
    scale_color_distiller(
                    direction = 1,
                    na.value = "grey90",
                    palette = "Blues"
    ) +
  # Fiddled with these numbers until the density plot aligned with legend
  scale_x_continuous(expand = expansion(mult = c(0.09, 0.19))) +
  scale_y_continuous(expand = expansion(mult = c(0.18, 0.1))) +
  # scale_x_continuous(expand = expansion(mult = c(0.11, 0.19))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(), 
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )


# ----- Paste everything together with library(patchwork) -------

design <- "
11111111111111111444
11111111111111111444
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111555
11111111111111111555
"    
bach_degree_map +
  guide_area() + 
  pct_bach_higher_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")



```

The most highly educated ZCTAs tend to be in East Oakland, West San Francisco, and South San Mateo. The least educated ZCTAs tend to be in South Oakland, and South San Francisco. Generally these maps track the median rent map in that highly educated ZCTAs tend to have high rents and vice versa. These two Census variables have a correlation of `r cor(acs_zcta_polygons %>% filter(in_study_area) %>% pull(pct_bach_higher_e), acs_zcta_polygons %>% filter(in_study_area) %>% pull(median_rent_e), use = "complete.obs") %>% round(2)`.

#### Interactive

```{r}
#| cache: false

acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  mapview(zcol = "pct_bach_higher_e",
          layer = "% Bachelors </br> or higher",
          col.regions=brewer.pal(7, "Blues"),
          na.color = "grey40") +
  mapview(acs_zcta_polygons %>% filter(!in_study_area),
          col.regions = "grey80",
          legend = FALSE)
  
```
:::

The main types of neighborhoods that we've identified in Map 2.1 and 2.2 are high housing costs and highly educated neighborhoods, which tend to be in downtown San Francisco and South San Mateo. I'll call these tech worker hubs, because these are neighborhoods close to where the headquarters of many Silicon Valley companies are located. It makes sense that high income and highly educated tech workers would drive up rental prices here. Similarly there are less college educated and lower housing cost neighborhoods in South San Francisco and most of Oakland. 

My hypothesis would be that Airbnbs cluster near these tech worker hubs (as many workers may need short term housing) and near downtown SF neighborhoods with high rental prices (as these locations are close to attractions and considered safe).

### Map 3: [Average Log Price]{style="color:#EF8A17"} of Airbnb listings and [Median Rent]{style="color:#2171b5"}

```{r create_airbnb_log_price_vs_median_rent_map}
#| cache: false
#| fig-height: 8.75


# ---- Do log correction -----
# For the 4 listings with price = 0, change to price = 1 so log is defined
airbnb_data_log_corrected = airbnb_points %>% 
  mutate(price = case_when(
    price == 0 ~ 1,
    TRUE ~ price
  ),
  # Separate price into quartiles
  price_ntile = ntile(price, 4)
  )



# ---- Prepare data for use in leaflet ------
acs_zcta_polygons_4326 = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  st_transform("EPSG:4326")

hex_grid_airbnb_variables_4326 = hex_grid_airbnb_variables %>% 
  filter(!is.na(avg_log_hex_price)) %>% 
  st_transform("EPSG:4326")

# Make color palettes
pal_fun_oranges <- colorNumeric("Oranges", NULL, na.color= "grey90")
pal_fun_gn_bu <- colorNumeric("GnBu", NULL, na.color= "grey90")
pal_fun_blues <- colorNumeric("Blues", NULL, na.color= "grey90")


# --- Calculate correlations -----

# Calculate listing level and zcta level correlations between price and median rent 
listing_level_correlation_price_rent = airbnb_data_log_corrected %>% 
  # this is price of listing, and median rent of zcta that listing is in
  select(price, 
         median_rent_e) %>% 
  st_drop_geometry() %>% 
  {cor(.$price, .$median_rent_e, use = "complete.obs")} %>% 
  round(2)

zcta_level_correlation_price_rent = airbnb_avg_price_by_zcta %>% 
  # this is avg price of listing in zcta, and median rent of zcta
  select(avg_price, 
         zcta) %>% 
  st_drop_geometry() %>% 
  left_join(acs_zcta_polygons %>% 
              st_drop_geometry() %>% 
              select(zcta, median_rent_e), 
            by = "zcta") %>% 
  {cor(.$avg_price, .$median_rent_e, use = "complete.obs")} %>% 
  round(2)


# --- Make leaflet map ----

# Make leaflet map
log_price_vs_rent_leaflet_map = leaflet() %>% addMapPane("background", zIndex = 400) %>%  
  addMapPane("foreground", zIndex = 500) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addPolygons(data = acs_zcta_polygons_4326, 
              group="Median Rent", 
              color ='white',
              fillOpacity=0.4 ,
              fillColor= ~pal_fun_blues(median_rent_e),
              weight = 1,
              # stroke = FALSE,
              options = pathOptions(pane = "background")) %>% 
  addPolygons(data = hex_grid_airbnb_variables_4326,
              group="Avg Log Price",
              color='grey90',
              fillOpacity=0.5,
              fillColor= ~pal_fun_oranges(avg_log_hex_price),
                # ~colorQuantile("Oranges", 
                #                  avg_log_hex_price,
                #                  na.color = "grey")(avg_log_hex_price),
              weight = 1,
              # stroke = FALSE,
              options = pathOptions(pane = "foreground")) %>%
  addLayersControl(overlayGroups = c("Median Rent", "Avg Log Price"),
                   options = layersControlOptions(collapsed = FALSE)) %>% 
  addLegend("bottomright",  # location
            pal=pal_fun_oranges,    # palette function
            values= hex_grid_airbnb_variables_4326$avg_log_hex_price,  # value to be passed to palette function
            title = 'Avg Ln(Price)',
            bins = seq(3,7, by = 1)) %>% 
  addLegend("bottomright",  # location
            pal=pal_fun_blues,    # palette function
            values= acs_zcta_polygons_4326 %>% filter(!is.na(median_rent_e)) %>% pull(median_rent_e),  # value to be passed to palette function
            labFormat = labelFormat(prefix = "$"),
            title = 'Median Rent'#,
            # bins = c(500,1000, 1500,2000, 2500, 3000, 3500)
            )


# ---- Make mapview map ----
mapview(acs_zcta_polygons %>% filter(in_study_area),
        zcol = "median_rent_e",
        col.regions = brewer.pal(6,"Blues"),
        layer.name = "Median Rent",
        alpha.regions = 0.4,
        col = "white") +
mapview(hex_grid_airbnb_variables %>% filter(!is.na(avg_log_hex_price)),
        zcol = "avg_log_hex_price",
        col.regions = brewer.pal(6, "Oranges"),
        layer.name = "Avg Log(Price)",
        alpha.regions = 0.5,
        color = "transparent",
        lwd = 0.2) 




```

Map 3 shows the average log price of Airbnb listings overlaid on top of a choropleth map of Median rent. Darker orange mean more expensive listings, and darker blue means higher median rent neighborhoods. For this map:

1)  I aggregate the 13,000+ Airbnb points into hexagon grids and presented the average log price of listings. Hexagons allow us to visualize relative density effectively within equal area grids and don't clutter up the map.

2)  I use interactivity because the ability to toggle layers on and off is very helpful when comparing two variables. If you hover over the layer button (right below the zoom button) and toggle the `Avg Log(Price)`, you can visually compare where the high priced airbnb neighborhoods overlap the high rent neighborhoods.

3)  I set increased transparency of the Median Rent variable to visually send that layer into the background.

4)  I turn the map title into a pseudo-legend by making use of colors to prime the reader for the two different variables scales on the map.

5)  There are 4 listings which have a list price of \$0 a night. I manually increment them to be \$1 a night so that  `log(price)` is well defined.

Neighborhoods with high listing prices (dark orange) tend to cluster near the coast, and not always near high rent (dark blue) neighborhoods. There are some clusters of high listing price neighborhoods close to high rent neighborhoods, particularly in San Mateo near the Silicon Valley belt. But the highest listing neighborhoods are not generally in the highest rent neighborhoods. The ZCTA level correlation between average listing price and median rent for the 167 ZCTAs in our study area is `r zcta_level_correlation_price_rent` and therefore confirms a weakly positive relationship between median ZCTA rent and Airbnb price.

## Potential Raster Datasets

Some raster datasets I could use to improve to this analysis would be:

1)  Air quality data that measures particulate matter or ozone at various grid sizes, like [these data](https://ww2.arb.ca.gov/geographical-information-system-gis-library) from the California Air Resources Board. I could overlay this on top of our listing data to see if more listings or more pensive listings are correlated with being in good air quality areas.

2)  Vegetation data/indexes, that measure tree canopy cover and greenspaces at various grid sizes, like [this raster data](https://gis.data.ca.gov/datasets/CDFW::naip-2020-ndvi-california/about) from the California Department of Fish and Wildlife that provides NVDI imagery at 60 cm resolution. I could use this to look at relationships between the amount of Airbnbs/Airbnb prices, socioeconomic variables, and the amount of greenspace and vegetation available in the neighborhood.

# Part 2: Spatial Analysis

I perform an analysis of travel time during rush hour by bike and car from a bikeshare station in central San Francisco at Union Square. The key question I will be asking is which trips are quicker by bike rather than car and why?

## Visualizing bike rental network

I create a heatmap of bike rentals pulled from OpenStreetMap. I make a few key decisions justified below:

1)  I use hexagon binning to aggregate the point level data and present zero bins as grey NA values to easily see the neighborhoods in SF without bike rentals.

2)  For exploratory purposes, I provide a point map of bike rentals in a separate tab which maps the point distributions. I overlay the full listing of Bay Wheels Bikeshare stations on this map (in light green) as calculated from the Bay Wheels data, which highlights the limitations of relying on crowdsourced and incomplete OSM data.

::: panel-tabset
### Heatmap

#### Map 4: Heatmap of [OSM Bike Rentals]{style="color:#611bb1"} in San Francisco

```{r}
#| cache: false

# ---- Crop ACS ZCTA polygons to San Francisco -----
acs_zcta_polygons_in_sf = acs_zcta_polygons %>% 
  mutate(zcta_area = st_area(.)) %>% 
  st_intersection(sfo_county_boundary %>% 
            select(county_name = name)) 

# ---- Make heatmap with leaflet -----
# leaflet() %>%
#   addProviderTiles(provider = providers$CartoDB.Positron) %>%
#   addHeatmap(data = bike_rental_points_sfo %>% st_transform("EPSG:4326"), 
#              blur = 25,
#              radius = 10,
#              gradient = "GnBu")


# ---- Make heatmap with hex bins -----

hex_grid_bike_variables %>%
  st_join(sfo_county_boundary, left = FALSE) %>%
  mapview(zcol = "num_stations",
          layer = "# Locations",
          alpha.regions = 0.6,
          # at = seq(0,20, by = 5),
          col.regions = brewer.pal(7, "Purples"))

```

<br>

Most bike stations are located in North East San Francisco, where the Financial Districts and major tourist attractions (like Pier 39) are located. There are basically no bike rental locations in West or South San Francisco, which makes sense as there are large mountains and hills in that part of the city.

### Point Map

#### Map 4: Point map of [OSM Bike Rentals]{style="color:#611bb1"} and [Bay Wheels stations]{style="color:#82d9b7"} in San Francisco

```{r make_point_density_map}

# Make point map of Bike rental stations
bike_point_map_sf = ggplot() +
  geom_sf(data = acs_zcta_polygons_in_sf,
          color = "white",
          fill = "grey90"
          ) +
  geom_sf(data = bike_rental_points_sfo, 
          color = "#6a51a3",
          stroke = FALSE,
          alpha = 0.7,
          size = 3) +
  geom_sf(data = end_stations,
          color = "#2D936C",
          stroke = FALSE,
          alpha = 0.2,
          size = 3) +
  theme_void() +
  scalebar(acs_zcta_polygons_in_sf, 
           dist = 1, 
           dist_unit = "km", 
           transform = FALSE,
           location = "topleft",
           height = 0.01,
           st.size = 4,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2)


bike_point_map_sf


```
:::

## Creating buffers

```{r creating_buffers}
#| cache: false

# ---- Create buffer around bike stations ----
bike_rental_points_buffered = bike_rental_points_sfo %>% 
  # filter(in_study_area) %>% 
  st_buffer(200)

airbnbs_within_200m_of_bikes = airbnb_points %>% 
  st_intersection(bike_rental_points_buffered)

# ---- Count unique Airbnbs ------
unique_airbnbs_in_buffer = airbnbs_within_200m_of_bikes %>% 
  st_drop_geometry() %>% 
  count(id, sort = TRUE)

count_airbnbs_close_to_bikes = nrow(unique_airbnbs_in_buffer)
count_total_airbnbs_in_sfo = nrow(airbnb_points %>% st_intersection(sfo_county_boundary))
```

I create 200 meter buffers around bike share stations in SF, and perform a spatial join to see which Airbnbs are bike accessible. Overall, `r count_airbnbs_close_to_bikes %>% format(big.mark = ",", scientific = FALSE)` unique Airbnbs (or `r ((count_airbnbs_close_to_bikes/ count_total_airbnbs_in_sfo) %>% round(3))*100`% of Airbnbs in San Francisco) are within the 200 meter buffers. This would help me decide where to rent an Airbnb, but I would have doubts about the effectiveness of this strategy given that we know OSM data is incomplete.

## Generating bike and car travel times

```{r, generate_summary_stats_for_bike_trips}
#| cache: true
#| echo: false

# ---- Generate summary stats for trips to other stations -----

# Takes a long time! Get group wise ntiles and number of trips
bike_trip_data_sfo = bike_trip_data_sfo %>% 
  group_by(start_station_id, end_station_id) %>% 
  mutate(duration_ntile = ntile(duration, 10),
         num_trips_between_stations = n()) %>% 
  ungroup()


# Helper fxn to calculate if value is in middle 90% quantile
is_in_bottom_80_quantile = function(x){
  x <= quantile(x, 0.8)
}

# Keep only bottom 80% of bike trips to remove very long trips for each station.
# Keep all data if there are less than 5 trips between origin and destination
bike_trip_data_sfo_trimmed = bike_trip_data_sfo %>% 
  group_by(station_combo_id) %>% 
  mutate(is_in_bottom_80_quantile = duration <= quantile(duration, 0.8)) %>% 
  filter(is_in_bottom_80_quantile | num_trips_between_stations<5) %>% 
  ungroup()

# From trimmed data, calculate avg and median duration
bike_trips_beween_stations_summary = bike_trip_data_sfo_trimmed %>% 
  group_by(start_station_id, end_station_id) %>% 
  summarize(num_trips_between_stations = n(),
            avg_duration = (mean(duration) %>% as.numeric(units = "hours"))*60*60,
            median_duration = (median(duration) %>% as.numeric(units = "hours"))*60*60
              ) %>% 
  ungroup()  %>% 
  # Append point geographies of end station
  left_join(sfo_end_stations, by = "end_station_id") %>% 
  st_as_sf()
 

```

Overall there are `r nrow(bike_trip_data_sfo_trimmed) %>% format(big.mark = ",", scientific = FALSE)` total trips from the Union Station dock in the Bay Wheels data, and `r nrow(bike_trips_beween_stations_summary) %>% format(big.mark = ",", scientific = FALSE)` unique trip combinations. For each unique trip combination, I calculate average (mean and median) trip durations of all trips in the Bay Wheels data. Before calculating averages, I drop the top 20% of longest trips for each trip combination. This is because the distribution of biking times has a long right tail, likely representing riders who got lost, or had trouble getting to the destination. These trips are not representative of average trips and heavily skew our grouped calculations, so I drop them. For trip combinations that have less than 5 data points, I don't do this trimming as this would mean dropping half or more of the data points. And with limited trip information, I want to use all available information.

```{r, generate_summary_stats_for_bike_trips_just_code}
#| cache: true
#| echo: false
#| eval: false

# ---- Generate summary stats for trips to other stations -----

# Takes a long time! Get group wise ntiles and number of trips
bike_trip_data_sfo = bike_trip_data_sfo %>% 
  group_by(start_station_id, end_station_id) %>% 
  mutate(duration_ntile = ntile(duration, 10),
         num_trips_between_stations = n()) %>% 
  ungroup()


# Helper fxn to calculate if value is in middle 90% quantile
is_in_bottom_80_quantile = function(x){
  x <= quantile(x, 0.8)
}

# Keep only bottom 80% of bike trips to remove very long trips for each station.
# Keep all data if there are less than 5 trips between origin and destination
bike_trip_data_sfo_trimmed = bike_trip_data_sfo %>% 
  group_by(station_combo_id) %>% 
  mutate(is_in_bottom_80_quantile = duration <= quantile(duration, 0.8)) %>% 
  filter(is_in_bottom_80_quantile | num_trips_between_stations<5) %>% 
  ungroup()

# From trimmed data, calculate avg and median duration
bike_trips_beween_stations_summary = bike_trip_data_sfo_trimmed %>% 
  group_by(start_station_id, end_station_id) %>% 
  summarize(num_trips_between_stations = n(),
            avg_duration = (mean(duration) %>% as.numeric(units = "hours"))*60*60,
            median_duration = (median(duration) %>% as.numeric(units = "hours"))*60*60
              ) %>% 
  ungroup()  %>% 
  # Append point geographies of end station
  left_join(sfo_end_stations, by = "end_station_id") %>% 
  st_as_sf()
 
```

Then I use `library(hereR)` to calculate travel times by bike and car for those 251 trip combinations during rush hour (8 AM EST on Dec 8th, 2022). I also generate 8 minute drive time isochrones to represent the areas that travelers could get to in 8 minutes from the chosen station.

```{r generate_travel_time_matrices}
#| cache: true


# ---- Choose 1 focus station ------ 
chosen_station = sfo_end_stations %>% 
  filter(end_station_id == chosen_station_id)
        
# Get lng/lat
start_lng_lat = chosen_station %>% 
  st_transform("EPSG:4326") %>%
  st_coordinates() %>%
  as.vector()


# ---- Generate isochrones from HERE and OSM -----
chosen_station_8_min_car_isochrone_here = isoline(
  poi = chosen_station,
  datetime = as.POSIXct("2022-12-08 17:00:00 EST"),
  routing_mode = "short",
  range = 8 * 60,
  transport_mode = "car",
  traffic = TRUE,
  optimize = "quality"
) %>% 
  st_transform("EPSG:7131")


chosen_station_5_min_car_isochrone_osm <- osrmIsochrone(
  loc = c(start_lng_lat[1], start_lng_lat[2]), 
  breaks = 5
  ) %>% 
  st_transform("EPSG:7131")

# Limit to stations within 5 min car drive
stations_within_5_min_car_drive = 
  st_intersection(
    sfo_end_stations,
    chosen_station_8_min_car_isochrone_here) 

# Remove stations that are within 250 meters
stations_within_5_min_car_drive_study_area = 
  st_difference(
    stations_within_5_min_car_drive,
    chosen_station %>% 
      st_buffer(250)
  )


# ---- Generate travel time matrices from HERE -----

sfo_end_stations = sfo_end_stations %>% 
  # output travel matrices have a dest_id col that corresponds to index of destinations/
  # I create ourselves here so I can easily join later
  mutate(dest_id = row_number())

driving_route_matrix = 
  route_matrix(
    origin = chosen_station,
    destination = sfo_end_stations,
    datetime = as.POSIXct("2022-12-08 08:00:00 EST"),
    routing_mode = "short",
    transport_mode = "car"
  ) %>% 
  as_tibble()

biking_route_matrix = 
  route_matrix(
    origin = chosen_station,
    destination = sfo_end_stations,
    datetime = as.POSIXct("2022-12-08 08:00:00 EST"),
    routing_mode = "short",
    transport_mode = "bicycle"
  ) %>% 
  as_tibble()

walking_route_matrix = 
  route_matrix(
    origin = chosen_station,
    destination = sfo_end_stations,
    datetime = as.POSIXct("2022-12-08 08:00:00 EST"),
    routing_mode = "short",
    transport_mode = "pedestrian"
  ) %>% 
  as_tibble()

# ---- Bind travel time matrices together -----

# Bind together all travel matrices and transform to a wide df for easy joining
route_matrices = bind_rows(driving_route_matrix, biking_route_matrix, walking_route_matrix)

route_matrices_wide = route_matrices %>% pivot_wider(
 id_cols =dest_id,
 names_from = mode,
 values_from  = c("distance", "duration")
)

routing_data_for_chosen_station = route_matrices_wide %>% 
  left_join(sfo_end_stations, by = "dest_id") %>% 
  rename(geometry_end = geometry) %>% 
  mutate(geometry_start = chosen_station$geometry)

# routing_data_for_chosen_station %>% st_write("data/here_routing_information.geojson", delete_dsn = TRUE)

# ---- Combine HERE and Bay Wheels data -----
bike_trips_beween_stations_summary = bike_trips_beween_stations_summary %>%
  # Rename to standardize column names
  rename(duration_bicycle_baywheels_avg = avg_duration,
         duration_bicycle_baywheels_median = median_duration)

routing_duration_summary = bike_trips_beween_stations_summary %>% 
  left_join(routing_data_for_chosen_station %>% 
              select(-dest_id, geometry_end), 
            by = "end_station_id") %>% 
  select(start_station_id, 
         end_station_id,
         num_trips_between_stations,
         duration_car,
         duration_bicycle,
         duration_bicycle_baywheels_avg,
         duration_bicycle_baywheels_median,
         duration_pedestrian, 
         everything())


routing_duration_summary_long = routing_duration_summary %>% 
  pivot_longer(
    cols = c(duration_car, 
             duration_bicycle, 
             duration_bicycle_baywheels_avg, 
             duration_bicycle_baywheels_median, 
             duration_pedestrian),
    names_to = "mode",
    values_to = "duration"
  ) %>% 
  mutate(mode = str_remove_all(mode, "duration_"),
         # Convert duration from seconds to minutes
         duration = duration/60) %>% 
  select(mode, duration, everything())


```

## Assessing validity of bike estimates

From our previous calculations, I have 2 estimates of biking time for each of our 251 trips. One estimate from the HERE API and another from the Bay Wheels data. I compare the two to essentially assess the real world validity of the HERE estimates by:

1)  Ordering the 251 unique trips from the least difference to the most difference between HERE and Bay Wheels data estimates
2)  Splitting the data up into 20 buckets or ntile
3)  Calculating a few averages for each ntile

The results are summarized in the below table

### Difference in routing times between HERE and Bay Wheels data

```{r create_routing_time_diff_table}

# ---- Create routing time summary table -----
routing_duration_diffs = routing_duration_summary %>% 
  select(end_station_id, 
         num_trips_between_stations,
         duration_bicycle, 
         duration_bicycle_baywheels_avg,
         duration_bicycle_baywheels_median,
         distance_bicycle,
         distance_car) %>% 
  mutate(diff_here_baywheels_avg = duration_bicycle - duration_bicycle_baywheels_avg,
         diff_here_baywheels_med = duration_bicycle - duration_bicycle_baywheels_median
         ) %>% 
  pivot_longer(
    cols = starts_with("diff"),
    names_to= "bay_wheels_stat",
    values_to = "duration_difference"
  )


routing_duration_diff_table = routing_duration_diffs %>% 
  filter(bay_wheels_stat == "diff_here_baywheels_med") %>% 
  mutate(ntile_duration_diff = ntile(duration_difference, 20),
         percent_rank_duration_diff = percent_rank(duration_difference)) %>% 
  group_by(ntile_duration_diff) %>% 
  summarize(
          avg_duration_here = mean(duration_bicycle) /60,
          avg_duration_system_data = mean(duration_bicycle_baywheels_median) /60,
          avg_duration_difference = mean(duration_difference) /60,
          avg_number_of_trips = mean(num_trips_between_stations) / 60,
          avg_distance_car = mean(distance_car),
          avg_distance_bicycle = mean(distance_bicycle)) %>% 
  select(Ntile = ntile_duration_diff,
         `Distance (m)` = avg_distance_bicycle,
         `# of Trips` = avg_number_of_trips,
         `HERE`  = avg_duration_here,
         `Bay Wheels Median`  = avg_duration_system_data,
         `Diff (min)` = avg_duration_difference,
         ) %>% 
  st_drop_geometry()


  gt_theme_538 <- function(data,...) {
    data %>%
    opt_all_caps()  %>%
    opt_table_font(
      font = list(
        google_font("Lato"),
        default_fonts()
      )
    ) %>%
      tab_style(
        style = cell_borders(
          sides = "bottom", color = "transparent", weight = px(2)
        ),
        locations = cells_body(
          columns = TRUE,
          # This is a relatively sneaky way of changing the bottom border
          # Regardless of data size
          rows = nrow(data$`_data`)
        )
      )  %>% 
    tab_options(
      column_labels.background.color = "white",
      table.border.top.width = px(3),
      table.border.top.color = "transparent",
      table.border.bottom.color = "transparent",
      table.border.bottom.width = px(3),
      column_labels.border.top.width = px(2),
      column_labels.border.top.color = "transparent",
      column_labels.border.bottom.width = px(2),
      column_labels.border.bottom.color = "black",
      data_row.padding = px(3),
      source_notes.font.size = 12,
      table.font.size = 12,
      heading.align = "left",
      ...
    ) 
}


routing_duration_diff_table %>% 
  gt() %>% 
  gt_theme_538 %>% 
  fmt_number(
    columns = -`Ntile`,
    rows = everything(),
    decimals = 2,
  ) %>% 
  tab_spanner(label = "Avg", 
              columns = -`Ntile`) %>% 
  data_color(
    columns = c(`Diff (min)`),
    colors = scales::col_numeric(
      palette = "PuOr",
      reverse = TRUE,
      domain = c(-25,25)
    )
  )



```

Overall the expected biking times between HERE and Bay Wheels data are within a few minutes of each other. For Ntiles 7-20 - which corresponds to 65% or 163/251 unique bike trips - the HERE API times are an overestimate, and real world bikers are a little faster. For Ntiles 1-7 - which corresponds to 35% of bike trips - The HERE API times are an underestimate. The travel times that HERE got the most wrong (ie the bottom and top of the table) tend to be routes with a low number of overall trips and relatively long distance trips. These are infrequent/rare trips and so our limited Bay Wheels data might be skewed.

## Analyzing travel times

Next I map our calculated travel times by mode of transport (car vs bike)

::: panel-tabset
### Total Time

#### Map 5.1: Travel Time Estimates from [Union Station]{style="color:#1B98E0"}

```{r make_travel_time_map}

# ---- Clean up data for map -----

routing_duration_summary_long_for_plot = routing_duration_summary_long %>% 
  # don't plot pedestrian numbers
  filter(mode != "pedestrian") %>% 
  # There are 542 rides that end at the same start station, which I remove
  filter(end_station_id != chosen_station$end_station_id) %>% 
  # Make text more readable
  mutate(mode = case_when(
    mode == "bicycle" ~ "Bike (HERE)",
    mode == "bicycle_baywheels_avg" ~ "Bike (Bay Wheels Avg)",
    mode == "bicycle_baywheels_median" ~ "Bike (Bay Wheels Median)",
    mode == "car" ~ "Car (HERE)"
  )) %>% 
  mutate(mode = fct_relevel(mode, 
                          levels = c("Bike (Bay Wheels Avg)", 
                                     "Bike (Bay Wheels Median)", 
                                     "Bike (HERE)", 
                                     "Car (HERE)")))

# ---- Make map -----
routing_duration_summary_long_for_plot %>% 
  ggplot() +
  geom_sf(data = sfo_county_boundary, 
          fill = "gray95", 
          color = "white",
          size = 3) +
  geom_sf(aes(color = duration, size = num_trips_between_stations),
          alpha = 0.5) +
  geom_sf(data = chosen_station, 
          color = "#1B98E0",
          size = 1.7,
          alpha  = 0.9) +
  scale_color_viridis_c(direction = -1, option = "magma") +
  theme_void() +
  facet_wrap(~mode) +
  guides(color = guide_colorbar(title = "Duration (min)"),
         size = guide_legend(title = "# Trips",
                             override.aes = list(color = "grey80",
                                                 alpha = 0.7))) +
  theme(
      legend.margin = margin(0,0,0,0),
      text = element_text(family = "Lato"),
      plot.title= element_text(face = "bold.italic", 
                               size = 13),
      strip.text = element_text(size = 12, 
                                vjust = 0.9,
                                face = "bold.italic"),
      # axis.line.y = element_line(),
      # axis.text.y = element_text(),
      # axis.text.x = element_text(),
      # axis.title.x = element_text(),
      plot.title.position = "plot") +
    scalebar(routing_duration_summary_long_for_plot, 
           dist = 1, 
           dist_unit = "km", 
           transform = FALSE,
           location = "topleft",
           height = 0.02,
           st.size = 3,
           st.color = "grey80", 
           box.fill = c("gray90", "gray98"),
           box.color = "gray90", 
           border.size = 0.2,
           facet.var = "mode",
           facet.lev = "Bike (Bay Wheels Avg)")



```

### Difference from Car

#### Map 5.2: Travel Time Difference relative to Car from [Union Station]{style="color:#1B98E0"}

```{r make_travel_time_diff_map}


# ---- Clean up data for map ------
routing_duration_diff_summary = routing_duration_summary %>% 
  mutate(
    bicycle = duration_bicycle - duration_car,
    bicycle_baywheels_avg = duration_bicycle_baywheels_avg - duration_car,
    bicycle_baywheels_median = duration_bicycle_baywheels_median - duration_car,
  ) %>% 
  select(end_station_id, 
         starts_with("bicycle"),
         num_trips_between_stations, 
         starts_with("distance")) %>% 
  pivot_longer(cols =c(bicycle, bicycle_baywheels_avg, bicycle_baywheels_median),
               names_to = "mode", 
               values_to = "duration_diff") %>% 
  # don't plot pedestrian numbers
  filter(mode != "pedestrian") %>% 
  # There are 542 rides that end at the same start station, which I remove
  filter(end_station_id != chosen_station$end_station_id) %>% 
  # Make text more readable
  mutate(mode = case_when(
    mode == "bicycle" ~ "Bike (HERE)",
    mode == "bicycle_baywheels_avg" ~ "Bike (Bay Wheels Avg)",
    mode == "bicycle_baywheels_median" ~ "Bike (Bay Wheels Median)")) %>% 
  mutate(mode = fct_relevel(mode, 
                          levels = c("Bike (Bay Wheels Avg)", 
                                     "Bike (Bay Wheels Median)", 
                                     "Bike (HERE)"))) %>% 
  mutate(duration_diff = duration_diff/60) %>% 
  mutate(is_negative = case_when(
    duration_diff <=0 ~ "— Diff",
    duration_diff >0 ~ "+ Diff"
  ))


routing_duration_diff_summary_in_8_min_car_isochrone = routing_duration_diff_summary %>% 
  st_intersection(chosen_station_8_min_car_isochrone_here)


# ---- Make map ------

routing_duration_diff_summary_in_8_min_car_isochrone %>% 
  ggplot() +
  geom_sf(data = chosen_station_8_min_car_isochrone_here, 
          fill = "gray95", 
          color = "white",
          size = 3) +
  geom_sf(aes(color = duration_diff, 
              size = num_trips_between_stations),
          alpha = 1) +
  scale_size_area(max_size = 3) +
  scale_color_distiller(
    palette = "PuOr",
    direction = -1,
    values = c(0,0.05, 1)
    # limits = c(-100, 100)
    ) +
  geom_sf(data = chosen_station, 
          color = "#1B98E0",
          size = 1.7,
          alpha  = 0.4) +
  theme_void() +
  facet_grid(cols = vars(mode),
             rows = vars(is_negative),
             switch = "y") +
  guides(color = guide_colorbar(title = "Diff (min)"),
         size = guide_legend(title = "# Trips",
                             override.aes = list(color = "grey80",
                                                 alpha = 0.7))) +
  theme(
      legend.margin = margin(0,0,0,0),
      text = element_text(family = "Lato"),
      plot.title= element_text(face = "bold.italic", 
                               size = 13),
      strip.text = element_text(size = 12, 
                                vjust = 0.9,
                                face = "bold.italic"),
      # strip.text.y = element_text(angle = 180),
      # strip.placement = "outside", 
      # axis.line.y = element_line(),
      # axis.text.y = element_text(),
      # axis.text.x = element_text(),
      # axis.title.x = element_text(),
      plot.title.position = "plot")



# ---- Calculating shortest and longest trips -----

# x = routing_duration_summary %>%
#   filter(end_station_id != chosen_station$end_station_id)

# x %>% 
#   arrange(duration_bicycle) %>% 
#   slice(1:3) %>% 
#   select(end_station_id, starts_with("duration"), starts_with("dist"))
# 
# x %>% 
#   arrange(-duration_bicycle_baywheels_avg) %>% 
#   slice(1:3) %>% 
#   select(end_station_id, starts_with("duration"), starts_with("dist"))
# 
# x %>% 
#   arrange(-duration_bicycle_baywheels_median) %>% 
#   slice(1:3) %>% 
#   select(end_station_id, starts_with("duration"), starts_with("dist"))
# 
# x %>%
#   arrange(-duration_car) %>% 
#   slice(1:3) %>% 
#   select(end_station_id, starts_with("duration"), starts_with("dist"))
# 
# x %>% filter(end_station_id == "SF-U21")  %>% 
#     select(end_station_id, starts_with("duration"), starts_with("dist")) %>% 
#   mapview() + mapview(chosen_station, col.regions= "red")
# 
# x %>% 
#   arrange(distance_bicycle) %>% 
#   slice(1:3) %>% 
#   select(end_station_id, starts_with("duration"), starts_with("dist"))
# 
# 
# x %>% 
#   filter(end_station_id %in% c("SF-Y7", "SF-B15")) %>% 
#   st_drop_geometry() %>% 
#   View()


# routing_duration_diff_summary %>% 
#   filter(duration_diff < 5)
# 
# 
# routing_duration_diff_summary_in_8_min_car_isochrone %>% 
#     filter(duration_diff <0) 
# 
# routing_duration_diff_summary_in_8_min_car_isochrone %>%
#   filter(mode == "Bike (Bay Wheels Median)") %>%
#   # mapview()
#   filter(duration_diff <3) %>%
#   mapview()


```
:::

In Map 5.1 we see that car trips to all stations are relatively quick. Bike times are comparable at short distances but not at long distances. The quickest trip from the Bay Wheels data is to station `SF-F27`, which is three blocks down the same street. The slowest trip is to station `SF-B15`, which is in the Presidio. This interestingly is not the longest distance trip but there is a large hill climb.

In Map 5.2 I only display stations within the 8 minute driving time isochrone and shows differences relative to car trips. 8/90 stations are faster by bike than car (ie have negative difference values) according to the Bay Wheels medians. If we expand this criteria to bike trips that are at most 3 minute slower than cars (perhaps taking into account parking time), this includes 62/90 stations. The majority of the 8 bike friendly destinations lie along Market Street and Polk Street, which are both main streets with fully protected grade separated bike lanes. The takeaway here for policy makers is that to incentivize speedy bike trips, build more and better protected bike lanes. Better bike infrastructure here translates to faster bike trips.

# Conclusion

This paper first analyzed Airbnb and Census data for San Francicso, Oakland, and
San Mateo. We found that expensive neighborhoods tend to also be highly educated 
neighborhoods. And there are clear tech worker hubs with large numbers of Airbnb 
listings in South San Mateo (near the headquarters of Silicon Valley companies)
and central San Francisco. The highest priced Airbnbs tend to be along 
coastlines and close to tourist amenities. Future analyses of the Airbnb data 
should take into account seasonality, and other rental/housing market data.

I then analyzed travel times by car and bike throughout San Francisco 
from a central bikeshare dock. There are many station trips that are quicker, 
or at least equally as fast as car, mostly located along major bikeways. Future
analyses of bikeshare data should take into account the locations of bike
lanes/bike infrastructure across San Francisco, days/times of trips, and
topography of the city.

# References

::: {#refs}
:::

# Appendix

## A note on ZCTAs

Zip Code Tabulation Areas (ZCTAs) are generalized representations of zip codes, and in most cases are very similar to zip codes. But their boundaries can sometimes be a little different due to differences in how the Postal Service and Census calculates zip codes vs ZCTAs respectively. For the sake of standardization, in this paper I always use ZCTA geographies instead of zip code geographies as these are the geography at which ACS data are available and are therefore a standard unit of geography we can join all our point and polygon level data to. Whenever I refer to "zip codes", these will always mean Zip Code Tabulation Areas. If I had used USPS zip code level geographies (like [this data](https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q) from Berkeley Geodata library) I would have to solve the complex problem of merging zip codes to ZCTA's and dealing with zip codes that fall into multiple ZCTA's.

## A note on the ACS

The 5 year ACS survey contains average values from 5 years of data collection in order to provide robust and precise estimates with low standard errors. The trade off is that these data are not the most recent and should instead be interpreted as roughly the average demographic composition within 2016-2020. This timeframe does not align with the Airbnb data which was scraped in 2022, but is likely still the best estimate of the demographic composition of these neighborhoods.