---
title: "Airbnb Geographic Analysis"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 7
    toc: true
execute:
  warning: false
  error: false
  message: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib

---


# Introduction

# Data Collection 

## 1.1 Collecting and importing the data


```{r libraries}
library(tidyverse)
library(sf)
library(tidylog)
library(osmdata)
library(mapview)
library(tmap)
# For painless filepath pointing
library(here)
library(janitor)
library(tidycensus)
library(leaflet.extras2)
library(paletteer)
library(ggpubr)
library(RColorBrewer)
library(patchwork)
library(grid)
library(geomtextpath)
library(ggh4x)
library(units)
library(nngeo)
# acs_vars = load_variables(year = 2020, dataset = "acs5")

```

```{r data_read_in}
#| output: false
#| cache: true

#----- Read in data ------------

# ---- Airbnb data

# Helper fxn to download csv.gz file from Inside Airbnb, unzip and read in
download_and_readin_inside_airbnb_data = function(url, city){
  
  # Set data filepath
  data_filepath = here(str_glue("data/raw-data/{city}.csv.gz"))
  
  # Download data directly from Inside Airbnb website
  dir.create(here("data/raw-data/"), showWarnings = FALSE)
  download.file(url, destfile = data_filepath)


  # Read in data
  read_csv(here(data_filepath)) %>% 
    st_as_sf(coords = c("longitude", "latitude")) %>% 
    # Since coords are lot/lng, this is EPSG 4326
    st_set_crs("EPSG:4326") %>% 
    # To get nice and easily workable column names
    janitor::clean_names() %>% 
    mutate(city = city)
}


# Download and read in Airbnb data
sfo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-francisco/2022-09-07/data/listings.csv.gz",
                            city = "sfo")
oakland_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/oakland/2022-09-18/data/listings.csv.gz",
                            city = "oakland")
san_mateo_data = download_and_readin_inside_airbnb_data(url =
                              "http://data.insideairbnb.com/united-states/ca/san-mateo-county/2022-09-19/data/listings.csv.gz",
                            city = "san_mateo")

# Bind all data into one long dataframe, which is safe bc we have added a "city" column for easy identificiation
airbnb_data = bind_rows(oakland_data, sfo_data, san_mateo_data)

# ---- Zip code data

# Download and read in zip code data
dir.create("data/raw-data/", showWarnings = FALSE)
# Get download URL for geojson by examining API request logs on the website:
# https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q

# Try downloading Berkeley zip codes. Sometimes we get a 500 error in which
# case we use backup local copy
fpath  = tryCatch(
  expr = {download.file("https://geodata.lib.berkeley.edu/download/file/ark28722-s7888q-geojson.json",
              destfile = here("data", "bay_area_zip_codes.geojson"))
    here("data","bay_area_zip_codes.geojson") 
    },
  warning = function(w) {
    here("data","bay_area_zipcodes.geojson") 
    },
  error = function(e) {
    here("data","bay_area_zipcodes.geojson") 
    }
)

zip_codes = st_read(fpath, quiet = TRUE) %>% 
  clean_names()

# ---- ACS and ZCTA data

# Use get_acs from tidycensus
# Census servers often go down and get_acs fails intermittenetly. So if that
# happens read in local copy as backup
acs_data_raw = tryCatch(
  expr = {
    suppressMessages(get_acs(
                       # Get zcta level variables
                       geography = "zcta",
                       # Got these variable names by perusing data.census.gov website
                       variables = c(median_rent = "B25031_001",
                                     median_rent_as_pct_of_income = "B25071_001",
                                     unemp_rate_16_plus = "S2301_C04_001",
                                     num_bach_higher = "B23006_023",
                                     num_25_64_for_bach_higher = "B23006_001"
                                     # median_rent_no_bedroom = "B25031_002",
                                     # median_rent_1_bedroom = "B25031_003",
                                     # median_rent_2_bedroom ="B25031_004",
                                     # median_rent_3_bedroom = "B25031_005"
                                     ),
                       year = 2020,
                       survey = "acs5",
                       output = "wide",
                       # Allows us to get ZCTA geometries as sf dataframes
                       geometry = TRUE,
                       quiet = TRUE
    ) %>%
  janitor::clean_names() %>%
  # Calculate pct bach or higher using num and denom
  mutate(pct_bach_higher_e = num_bach_higher_e/num_25_64_for_bach_higher_e,
         pct_bach_higher_m = moe_ratio(
           num = num_bach_higher_e,
           denom = num_25_64_for_bach_higher_e,
           moe_num = num_bach_higher_m, 
           moe_denom = num_25_64_for_bach_higher_m
         )) %>%
  rename(zcta = geoid))
  },
  error = function (e) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  },
  warning = function (w) {
    st_read("data/acs_zcta_data_raw_saved.geojson", quiet = TRUE)
  }
  )


# We will use acs_data in appendix to see how boundaries
acs_data = acs_data_raw %>% 
  st_drop_geometry()

# --- OSM Data

# build a query
query <- opq(bbox = "San Francisco Bay Area") |> 
  add_osm_feature(key = "amenity", value = "bicycle_rental")

bike_rental_locations_osm <- osmdata_sf(query)

bike_rental_points = bike_rental_locations_osm$osm_points %>% 
  as_tibble() %>% 
  st_as_sf()

```



# Data Discussion 

## Dataset Overview
I make use of 4 datasets in this project:

1) Airbnb data: This is a periodically published snapshot of Airbnb listings in various global cities produced by Murray Cox on his website [Inside Airbnb](http://insideairbnb.com/). We programmatically download the zipped csv files containing listing data for San Francisco, San Mateo and Oakland, which will be our study area. The data for these three cities were scraped on 2022-09-07, 2022-09-19, and 2022-09-18 respectively and contains variables such as price, number of beds/baths, and review information for each of the `r nrow(airbnb_data)` total listings.

2) ACS Demographic Data: Using `library(tidycensus)` we download demographic information for each Zip Code Tabulation Area (ZCTA) in the Bay Area from the 2016-2020 5 year ACS. Specifically we look at the Median Rent per bedroom, and the percentage of adults with at least a bachelors degree. These variables give us a rough estimate about the housing costs and education levels within each Bay Area neighborhood. The percentage of adults with at least a bachelor's degree was calculated from Census Table B23006. And in line with Census recommendations, we use the number of adults over the age of 25 for the denominator of this percentage. 

The 5 year ACS survey contains average values from 5 years of data collection in order to provide robust and precise estimates with low standard errors. The tradeoff is that these data are not the most recent and should instead be interpreted as roughly the average demographic composition within 2016-2020. This timeframe does not align with the airbnb data which was scraped in 2022, but is likely still the best estimate of the demographic composition of these neighborhoods.

ZCTAs are generalized representations of zip codes, and in most cases are very similar to zip codes . But their boundaries can sometimes be a little different due to differences in how the Postal Service and Census calculates zip codes vs ZCTAs respectively (see Appendix A for a detailed comparison). For the sake of standardization, in this paper we will always use ZCTA geographies instead of zip code geographies as these are the geography at which ACS data are available and are therefore a standard unit of geography we can join all our point and polygon level data to. And whenever I refer to "zip codes", these will always mean Zip Code Tabulation Areas. If we had used USPS zip code level geographies (like [this data](https://geodata.lib.berkeley.edu/catalog/ark28722-s7888q) from Berkeley Geodata library) we would have to find a way to merge zip codes to ZCTA's and deal with zip codes that fall into multiple ZCTA's when working with ACS data. 


3) OSM Bike Rental Location : Using the overpass API and `library(osmdata)` in R, we download OSM Amenity data on bike rental stations in the Bay Area. These are user reported point locations of where bike stations are located in Open Street Map and include both private and municipial bike rental locations. 

## Data Cleaning

For the Airbnb data, we convert the `price` column from a character vector into a numeric vector by removing commas, and `$` signs from the column. This will allow us to treat this column as numeric when performing analyses later. After some exploratory analysis, we also decide to drop the top 0.5% most expensive Airbnb listings from this analysis as they are very high outliers. This corresponds to dropping 67 listings with prices greater than or equal to $2500 per night. Upon visual inspection see that most of these listing don't have any reviews and may not be real, justifying our decision.



```{r cleaning_data}
#| cache: true
#| output: false

# -----  Clean data ---------

# Clean up airbnb data
airbnb_data_cleaned = airbnb_data %>% 
  # Convert price from chr to numeric by removing $ and , characters
  mutate(price = str_replace_all(price, "\\$","") %>% 
           str_replace_all( ",", "") %>% 
           as.numeric()) %>% 
  select(id, price, listing_url, name, city, neighbourhood, neighbourhood_cleansed, property_type, bedrooms, bathrooms,number_of_reviews, has_availability, number_of_reviews, reviews_per_month, everything())



# -----  Drop Outliers ---------

# Drop 0.5% of highest priced outlier airbnb listings. 
# Top 0.5% of listings is effectively 67 listings with prices >= $2500
# Many of these outliers don't have any reviews and suggest they can be safely
# removed. There are 4 listings with a listed price of 0, but 2 do have
# reviews so we decide to keep them in
top_outlier_listings = airbnb_data_cleaned %>% 
  slice_max(price, 
            prop = 0.005) 

airbnb_data_cleaned = airbnb_data_cleaned %>% 
  # This effectively removes top 0.5% of highest priced listings, which is 
  # 67/13415 listings. Many of these outliers don't have any reviews and suggest
  # they can be safely removed. There are 4 listings with a listed price of 0, 
  # but 2 do have reviews so we decide to keep them in
  filter(price < 2500)


# Make histogram of dropped outliers
outlier_dropped_listings_price_histogram = top_outlier_listings %>% 
  ggplot(aes(x = price)) +
  geom_histogram(fill = "forestgreen", color = "white") +
  labs(title = "Histogram of Prices for top 0.5% of listings",
       x = "Price ($)", y = "Count (n)")

outlier_dropped_listings_price_histogram


```




## Projections 
I will use `EPSG:7131`or the NAD83 State Plane projection for San Franscisco for all our datasets as this was one of two projected coordinate reference systems specifically created by the San Francisco City and County GIS department. This is is a projected 2D conformal coordinate system that:

- preserves angles between locations
- is accurate for distance based calculations like buffers and travel time
- is suitable for the entirety of the Bay Area.



```{r projecting_data}
#| cache = TRUE

airbnb_data_projected = airbnb_data_cleaned %>% 
  st_transform("EPSG:7131")

zip_codes = zip_codes %>% 
  st_transform("EPSG:7131")

acs_data_raw = acs_data_raw %>% 
  st_transform("EPSG:7131")

bike_rental_points = bike_rental_points %>% 
  st_transform("EPSG:7131")
```




## Data Merging

We then append ZCTA information for each listing by spatially joining the point level Airbnb listings to the ZCTA polygons provided by the Census. This spatial join allows us to succesfully assign ZCTA for all but 24 of our listings. Upon visual inspection, the remaining 24 listings seem to be just outside the ZCTA boundaries. In one case, the listing was a yacht that was a few hundred meters off the coast. In order to not lose valuable information on these properties, we perform a second distance based spatial join to see if the listings are within 500 meters of any ZCTAs. With this 500 meter buffer, the 24 remaining listings were also assigned a ZCTA. 


```{r append_zcta_info_to_listings}
#| cache = TRUE

# ----- Append ZCTA information to listings -------

# Perform spatial join, which accurately joins most listings to a zcta
airbnb_data_joined = airbnb_data_projected %>% 
  # st_join(zip_code_demographic_data %>% select(zip, po_name), 
  #         left_join = TRUE) %>% 
  st_join(
    acs_data_raw %>% 
      select(zcta, median_rent_e, pct_bach_higher_e, unemp_rate_16_plus_e),
    left = TRUE
  )


# There are 24 listings which aren't joined to a zcta, lets instead match those 
# if they're within 500 meters of a zcta. This allows all the listings to be 
# assigned to a zcta succesfully
problematic_listings_matched_zctas = airbnb_data_joined %>% 
  filter(is.na(zcta)) %>% 
  # Drop zcta colummn so we can take it from the acs_data_raw df after join
  select(-zcta) %>% 
  st_join(acs_data_raw %>% 
            select(zcta),
          left = TRUE,
          # Use nearest neighbor join to return closest zcta to point
          nngeo::st_nn, 
          # Set max distance of 500 meters
          maxdist = 500, 
          k = 1,
          progress = FALSE
  )


# Remove unmatched listings from data, and append with newly matched listings
airbnb_data_corrected = airbnb_data_joined %>% 
  filter(!is.na(zcta)) %>% 
  bind_rows(problematic_listings_matched_zctas)

```


Finally, the ZCTA level ACS data that `library(tidycensus)` produces is for the entirety of the US. But will be making ZCTA level maps of the Bay Area and therefore need to crop our data. We crop the ZCTA level ACS data to a study area bounding box, which was created by:

1) Calculating which ZCTAs our Airbnb listings fall into
2) Generating a bounding box around those ZCTAs
3) Expanding those bounding boxes by 1-5% to add context on surrounding ZCTAs and make the region more square for easier visualization

This gives us a rectangular region, which contains all the relevant ZCTAs with Airbmb listings and a few other ZCTAs to help provide context on surrounding neighborhoods and be used as a basemap layer. 




```{r define_study_area_bbox}
#| cache: true

# ---- Define Study Area -------

# Get shapefiles of all ZCTAs with at least 1 airbnb
acs_zctas_with_airbnbs = acs_data_raw %>% 
  filter(zcta %in% unique(airbnb_data_corrected$zcta) ) 

# Define study area to be the bbox of the zctas with at least 1 airnb
study_area_box = acs_zctas_with_airbnbs %>%
  st_bbox() 

# Expanding bounding box by a little bit to provide more 
# context and to make map shape more square
xrange <- study_area_box$xmax - study_area_box$xmin # range of x values
yrange <- study_area_box$ymax - study_area_box$ymin # range of y values

study_area_box[1] <- study_area_box[1] - (0.25 * xrange) # xmin - left
study_area_box[2] <- study_area_box[2] - (0.05 * yrange) # ymin - bottom
study_area_box[3] <- study_area_box[3] + (0.15 * xrange) # xmax - right
study_area_box[4] <- study_area_box[4] + (0.05 * yrange) # ymax - top

# ---- Cropping ZCTA data to study area ------

# Limit ZCTA data to study area
acs_data_cropped = acs_data_raw %>% 
  st_crop(study_area_box)

# ---- Define final datasets -----
airbnb_points = airbnb_data_corrected
acs_zcta_polygons = acs_data_cropped



```


## Mapping/Data Viz

First we map the count (Map 1.1) and average price (Map 2.2) of Airbnb listings in Bay Area Zip Codes. Since we have already performed a spatial join to append the ZCTA information for each listing, we simply have to count the total number of listings to obtain the total count, and perform a grouped calculation by ZCTA to obtain the average price of listings. I made several design decisions for maps 1.1 and 1.2 which are justified below:

1) I decide to bin our continuous data into intervals because the ranges of our count and price variables are relatively large and we want to avoid having too many colors as we might have with a continuous color ramp. And for now, we just want to see
I also use manually created equal interval bins in order to make the bins human readable and give a reader an honest view of the data.

2) I use the readymade Green-Blue color palette from [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=GnBu&n=6), based on Cynthia Brewer's research [-@harrower2003colorbrewer] on gradients that are colorblind friendly, and suitable for LCD computer screens.

3) I include the number of zip codes that fall in each bin in the form of a histogram to the right of the legend. This is an effective way to give the reader the actual distribution of the variable and give  context on what may seem like arbitrary bin cutoff decisions. This also helps alleviate the problem of having areally large ZCTAs with low/high values bias the reader into thinking those high/low values are more common than they actually are. 

4) I choose to include the surrounding ZCTA boundaries as a basemap layer and do not actually map their values (which in this case would be 0 becuase there are no Airbmbs in these surrounding ZCTAs). I intentionally do not list these light gray ZCTAs in the legend as I do not want the reader to focus on these neighborhoods. They are there simpply to frame the outline of the Bay Area and perhaps serve as helpful geographic context for readers familiar with the Bay Area.

5) Finally we also provide interactive maps using `library(mapview)` for exploratory purposes. 


### Map 1.1: Count of Airbnb listings

::: {.panel-tabset}

#### Static 

```{r listings_by_zipcode_map}
#| fig-width: 7
#| fig-height: 7


# ----- Count airbnb listings by zcta ----------
airbnb_count_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  count(zcta, sort = TRUE) %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(n, zcta, geometry) %>% 
  st_as_sf()


# Create buckets for price for use in histogram
airbnb_count_by_zcta = airbnb_count_by_zcta %>% 
  mutate(count_buckets = case_when(
    n >=0 & n <75 ~ "0 - 75",
    n >=75 & n <150 ~ "75 - 150",
    n >=150 & n <225 ~ "150 - 225",
    n >=225 & n <300 ~ "225 - 300",
    n >=300 & n <375 ~ "300 - 375",
    n >=375 & n <450 ~ "375 - 450",
    n >=450 ~ "450 +"
  ),
  count_buckets =
    as_factor(count_buckets) %>%
    fct_relevel(c("0 - 75", 
                  "75 - 150",
                  "150 - 225",
                  "225 - 300",
                  "300 - 375",
                  "375 - 450",
                  "450 +"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_count_by_zcta %>% 
      st_drop_geometry() %>% 
      select(num_listings = n, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------


listing_bucket_counts = airbnb_count_by_zcta %>% 
  filter(!is.na(count_buckets)) %>%
  st_drop_geometry() %>% 
  count(count_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    count_buckets == "450 +"  ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

# Create histogram which will double as legend
num_listings_histogram_legend = listing_bucket_counts %>% 
  ggplot(aes(x = n, y = count_buckets, fill = count_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.275, "in"),
        legend.key.width = unit(0.05, "in"),
#         legend.key.height = unit(0.233, "in"),
#         legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold.italic", 
                                 size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "# Listings",
       x = "Count of Zip Codes")


# ----- Create choropleth map ----------
# Make map
airbnb_listings_by_zips_plot = airbnb_count_by_zcta %>% 
  ggplot(aes(fill = count_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none") +
  theme(plot.margin = margin())




# ----- Paste together with patchwork --------

design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_listings_by_zips_plot + 
  num_listings_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```


Most ZCTAs have 0-75 listings. The ZCTAs with the highest count of listings are concentrated in downtown San Francisco, right next to the Bay in West Oakland. And the least amount of Airbnbs tend to be in far east Oakland and the Southwest Bay Area. This makes intuitive sense as these locations are relatively far from common tourist destinations in San Francisco and Oakland. 

#### Interactive

```{r}
mapviewOptions(basemaps = "CartoDB.Positron")
mapview(airbnb_count_by_zcta, 
        zcol = "count_buckets", 
        layer = "Count of Listings",
        col.regions=brewer.pal(7, "GnBu"))
```



:::


### Map 1.2: Average Airbnb Prices in Bay Area Zipcodes

::: {.panel-tabset}

#### Static


```{r listing_avg_price_by_zip_map}
#| fig-width: 7
#| fig-height: 7

# ----- Calculate avg price by zcta ----------

airbnb_avg_price_by_zcta = airbnb_points %>% 
  st_drop_geometry() %>% 
  group_by(zcta) %>% 
  summarize(avg_price = mean(price)) %>% 
  ungroup() %>% 
  right_join(acs_zcta_polygons, 
             by = c("zcta")) %>% 
  select(avg_price, zcta, geometry) %>% 
  st_as_sf()

# Look at histogram for designing cut points
# airbnb_avg_price_by_zcta$avg_price %>% hist()

# Create buckets for price for use in histogram
airbnb_avg_price_by_zcta = airbnb_avg_price_by_zcta %>% 
  mutate(avg_price_buckets = case_when(
    avg_price >=0 & avg_price <75 ~ "$0 - $75",
    avg_price >=75 & avg_price <150 ~ "$75 - $150",
    avg_price >=150 & avg_price <225 ~ "$150 - $225",
    avg_price >=225 & avg_price <300 ~ "$225 - $300",
    avg_price >=300 & avg_price <375 ~ "$300 - $375",
    avg_price >=375 & avg_price <450 ~ "$375 - $450",
    avg_price >=450 & avg_price <700 ~ "$450 - $700",
  ),
  avg_price_buckets =
    as_factor(avg_price_buckets) %>%
    # fct_expand("$500 - $600") %>%
    fct_relevel(c("$0 - $75", 
                  "$75 - $150",
                  "$150 - $225",
                  "$225 - $300",
                  "$300 - $375",
                  "$375 - $450",
                  "$450 - $700"
                  ))
  )

# Append num listings per zcta back to zcta dataframe
acs_zcta_polygons = acs_zcta_polygons %>% 
  left_join(
    airbnb_avg_price_by_zcta %>% 
      st_drop_geometry() %>% 
      select(avg_price, 
             zcta),
    by = "zcta"
  )

# ---- Create histogram legend for map -------

avg_price_bucket_counts = airbnb_avg_price_by_zcta %>% 
  filter(!is.na(avg_price_buckets)) %>%
  st_drop_geometry() %>% 
  count(avg_price_buckets, .drop = FALSE) %>% 
  mutate(text = case_when(
    avg_price_buckets == "$450 - $700" ~ paste0(n, " Zips"),
    TRUE ~ as.character(n)
   ))

avg_price_histogram_legend = avg_price_bucket_counts %>% 
  ggplot(aes(x = n, y = avg_price_buckets, fill = avg_price_buckets)) +
  geom_col() +
  scale_x_continuous(expand = expansion(add = c(0, 7))) +
  # Ensure legend lines up with axis buckets, took a looot of fiddling
  scale_y_discrete(expand = expansion(mult = c(0.1, 0.11))) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = -11,
                    drop = FALSE) +
  geom_text(aes(label = text),
            hjust = -0.1,
            size = 3,
            color = "grey60",
            family = "Lato") +
  theme_void() +
  guides(fill = guide_legend(reverse = TRUE,
                             label.position = "left",
                             byrow = TRUE)) +
  theme(
        legend.title = element_blank(),
        legend.key.height = unit(0.275, "in"),
        legend.key.width = unit(0.05, "in"),
        legend.position = "left",
        legend.spacing.y = unit(0.015, "in"),
        legend.margin = margin(0,0,0,0),
        text = element_text(family = "Lato"),
        plot.title= element_text(face = "bold.italic",
                                 size = 13),
        # axis.line.y = element_line(),
        # axis.text.y = element_text(),
        # axis.text.x = element_text(),
        # axis.title.x = element_text(),
        plot.title.position = "plot") +
  labs(title = "Avg Listing Price",
       x = "Count of Zip Codes")



# ----- Create choropleth map ----------
  
# Make map
airbnb_avg_price_by_zips_plot = airbnb_avg_price_by_zcta %>% 
  ggplot(aes(fill = avg_price_buckets)) + 
  geom_sf(color = "white", size = 0.1) +
  scale_fill_brewer(na.value = "grey90",
                       palette = "GnBu",
                       direction = 1) +
  theme_void() +
  guides(fill = "none")


# ----- Paste together with patchwork --------


design = "
111111111111111111333
111111111111111111333
111111111111111111222
111111111111111111222
111111111111111111444
111111111111111111444
"

airbnb_avg_price_by_zips_plot + 
  avg_price_histogram_legend +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design)

```


Most Airbnbs tend to be between \$150 - \$300 a night. The highest priced Airbnbs tend to be along the Pacific Ocean coast, and along the coastline of San Francisco. This could be because these Airbnbs have great views, or access to beaches/other tourist friendly amenities. Conversely, the lowest priced Airbnbs tend to be Oakland. 

#### Interactive

```{r}

mapview(airbnb_avg_price_by_zcta, 
        zcol = "avg_price_buckets", 
        layer = "Avg Listing Price",
        col.regions=brewer.pal(7, "GnBu"))
```


:::

We next map our two Census variables at ZCTA level to see how they vary in our study area. Our maps are very similar to the previous maps except with a few different design decisions:

1) I now use a continuous color ramp instead of a binned color ramp. I do this because now we are interested in looking at small differences in the Census variables across ZCTAs, which continuous color ramps can do effectively.

2) I also include a color matched density plot (estimated using Kernel Density Estimation) to the right side of the legend. This is essentially a smoothed version of a histogram and allows the reader to get a sense of the distribution of the continuous variable. We choose to omit the axis for the density plot as actual density values can be hard to interpret (they are just curves whose area integrates to 1) and we really just want the reader to focus on the overall distribution of the variables and see that middle values are a lot more likely than extreme values. 


### Map 2.1: Median Rent by ZCTA

::: {.panel-tabset}

#### Static


```{r median_rent_map}
#| fig-width: 6
#| fig-height: 7.25

# Create indicator for whether a zcta is in study area or not
acs_zcta_polygons = acs_zcta_polygons %>% 
  mutate(in_study_area = case_when(
    !is.na(num_listings) ~ TRUE,
    is.na(num_listings) ~ FALSE,
    TRUE ~ NA
  ))


# ----- Create raw median rent sf ggplot -------
median_rent_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = median_rent_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_distiller(
                    labels = scales::dollar_format(),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      ),
                    palette = "GnBu"


  ) +
  # Hacky solution to force ggplot to include NA in legend scale, create 
  # blank color aesthetic for a tiny zcta, and then override aesthetics in 
  # guide to match NA colro values
  geom_sf(data= acs_zcta_polygons %>%
          filter(zcta == 94517),
          size = 0.1,
        aes(color = "NA")) +
  scale_color_manual(values = NA) +
  guides(colour=guide_legend("", 
                             override.aes=list(colour="grey70", fill = "grey70"),
                             label.position = "left",
                             order =2)) +
  geom_sf(data = acs_zcta_polygons %>% 
          filter(!in_study_area),
        fill = "grey90",
        color = "white") +
  labs(fill = "Median Rent") +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      # legend.box.just = "right",
      # legend.text.align = 0
      )

# median_rent_map

# ----- Generate density histogram for median rent -------

# Generate density matrix for our variable, and convert into df
median_rent_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(median_rent_e) %>% 
  na.omit() %>% 
  stats::density()

median_rent_density_df = tribble(~median_rent_e, ~density,
       median_rent_density$x, median_rent_density$y) %>% 
  unnest(cols = c(median_rent_e, density))


median_rent_density_line = median_rent_density_df %>% 
  ggplot(aes(x = median_rent_e,
             y = density,
             color = median_rent_e,
             # fill = median_rent_e
             )) +
  geom_line(lwd = 1.5, alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.1,
                hjust = 0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
  scale_color_distiller(
                  direction = 1,
                  na.value = "grey90",
                  palette = "GnBu"
  ) +
  # scale_color_viridis_c(
  #                 # labels = scales::percent_format(),
  #                 direction = 1,
  #                 na.value = "grey90"
  #                 ) +
  scale_x_continuous(expand = expansion(mult = c(0.065, 0.23))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )
# median_rent_density_line
# ----- Paste everything together with library(patchwork) -------

design <- "
11111111111111111444
11111111111111111444
11111111111111111633
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111555
11111111111111111555
"    
median_rent_map +
  guide_area() + 
  median_rent_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")


```


The ZCTAs with the highest rent tend to be in downtown San Fransisco and Southeast San Mateo. The ZCTAs with the lowest rent tend to be in Southeast San Francisco, and all throughout Oakland. The density plot shows us that most ZCTAs have average rents between \$1500 and \$2500 per bedroom.

#### Interactive

```{r}
acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  mapview(zcol = "median_rent_e",
          layer = "Median Rent",
          col.regions=brewer.pal(7, "GnBu"),
          na.color = "grey40") +
  mapview(acs_zcta_polygons %>% filter(!in_study_area),
          col.regions = "grey80",
          legend = FALSE)
  
```


:::

### Map 2.2: Percentage of 25+ population with Bachelors Degree or Higher

::: {.panel-tabset}

#### Static

```{r pct_bach_map}
#| fig-width: 6
#| fig-height: 7.25


# ----- Create raw % Bachelors sf ggplot -------

bach_degree_map = acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  ggplot(aes(fill = pct_bach_higher_e)) +
  geom_sf(color = "white", 
          size = 0.1) +
  scale_fill_distiller(
                    labels = scales::percent_format(),
                    limits = c(0,1),
                    direction = 1,
                    na.value = "grey70",
                    guide = guide_colorbar(
                      title.vjust = 6,
                      label.position = "left",
                      barheight = 14.5,
                      order =1
                      ),
                    palette = "GnBu"
                    
    
  ) +
  geom_sf(data = acs_zcta_polygons %>% 
            filter(!in_study_area),
          fill = "grey90",
          color = "white") +
  labs(fill = str_wrap(" % Bach. " ))  +
  # scale_fill_steps(high = "magenta", low = "blue", na.value = "grey90") +
  theme_void()  +
  theme(text = element_text(family = "Lato"),
      legend.title = element_text(size = 14, face = "bold.italic"),
      legend.text = element_text(size = 10),
      legend.box.just = "right",
      legend.text.align = 0
      # legend.title = element_blank()
      )

# ----- Generate density histogram for % bach -------

# Generate density matrix for our variable, and convert into df
pct_bach_higher_density = acs_zcta_polygons %>% 
  filter(in_study_area) %>%
  pull(pct_bach_higher_e) %>% 
  na.omit() %>% 
  stats::density()

pct_bach_higher_density_df = tribble(~pct_bach_higher_e, ~density,
       pct_bach_higher_density$x, pct_bach_higher_density$y) %>% 
  unnest(cols = c(pct_bach_higher_e, density))


# Create density histogram for our variable using ggplot and geom_line
# We need to use geom_line so we can get colors to vary along the line to match
# the legend coloring.
pct_bach_higher_density_line = pct_bach_higher_density_df %>% 
  ggplot(aes(x = pct_bach_higher_e,
             y = density,
             color = pct_bach_higher_e,
             )) +
  geom_line(lwd = 1.5, 
            alpha = 0.8) +
  geom_textpath(label = "Density",
                size = 4, 
                text_only = TRUE,
                color = "grey60",
                vjust = -0.3,
                family = "Lato",
                fontface = "bold.italic",
                alpha = 0.6
                )+
    scale_color_distiller(
                    direction = 1,
                    na.value = "grey90",
                    palette = "GnBu"
    ) +
  # Fiddled with these numbers until the density plot aligned with legend
  scale_x_continuous(expand = expansion(mult = c(0.09, 0.19))) +
  scale_y_continuous(expand = expansion(mult = c(0.18, 0.1))) +
  # scale_x_continuous(expand = expansion(mult = c(0.11, 0.19))) +
  coord_flip() +
  theme_void() +
  guides(color = "none") +
  theme(
    plot.margin = unit(c(0,0,0,0), "inches"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(), 
    axis.line = element_blank(),
    axis.ticks.length =  unit(0, "pt")
  )


# ----- Paste everything together with library(patchwork) -------

design <- "
11111111111111111444
11111111111111111444
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111233
11111111111111111555
11111111111111111555
"    
bach_degree_map +
  guide_area() + 
  pct_bach_higher_density_line + 
  plot_spacer() +
  plot_spacer() +
  plot_layout(design=design, guides = "collect")



```


The most highly educated ZCTAs tend to be in East Oakland, West San Francisco, and South San Mateo. The least educated ZCTAs tend to be in South Oakland, and South San Francisco. Generally these maps  track the median rent map in that highly educated ZCTAs tend to have high rents and vice versa. In fact these two Census variables have a correlation  of `r cor(acs_zcta_polygons %>% filter(in_study_area) %>% pull(pct_bach_higher_e), acs_zcta_polygons %>% filter(in_study_area) %>% pull(median_rent_e), use = "complete.obs") %>% round(2)`. 

#### Interactive

```{r}
acs_zcta_polygons %>% 
  filter(in_study_area) %>% 
  mapview(zcol = "pct_bach_higher_e",
          layer = "% Bachelors </br> or higher",
          col.regions=brewer.pal(7, "GnBu"),
          na.color = "grey40") +
  mapview(acs_zcta_polygons %>% filter(!in_study_area),
          col.regions = "grey80",
          legend = FALSE)
  
```


:::

The main types of neighborhoods that we've identified in Map 2.1 and 2.2 are high housing costs and highly educated neighborhoods, which tend to be in downtown San Francisco and South San Mateo. I'll call these tech worker hubs, because these are neighborhoods close to where the headquarters of many Silicon Valley companies are located. And it makes sense that high income and highly educated tech workers would drive up rental prices here. Similarly there are less college educated and lower housing cost neighborhoods in South San Francisco and most of Oakland. This is generally where the lower-income, and often Black and brown residents of the Bay tend to live. 

If I had to use this classification to evaluate where Airbnbs would cluster, my hypothesis would be that they would cluster near these tech worker hubs (as many workers may actually use Airbnb for short term housing) and near downtown San Francisco with high rental prices (as these locations are close to many tourist attractions and probably considered safe neighborhoods).

### Map 3: <span style="color:#EF8A17">Log Price</span> of Airbnb listings and <span style="color:#2171b5">Median Rent</span>



```{r}

# For the 4 listings with price = 0, change to price = 1 so log is defined
airbnb_data_log_corrected = airbnb_data_corrected %>% 
  mutate(price = case_when(
    price == 0 ~ 1,
    TRUE ~ price
  ))

# Make point plot + polygon plot of ln price vs median rent
ggplot() +
  geom_sf(data = acs_zcta_polygons %>% filter(!in_study_area),
          color = "white",
          fill = "grey90"
          ) +
  geom_sf(data = acs_zcta_polygons %>% filter(in_study_area),
          aes(fill = median_rent_e),
          color = "white",
          alpha = 0.65
          ) +
  scale_fill_distiller(na.value = "grey40",
                       palette = "Blues",
                       direction = 1,
                       labels = scales::dollar_format()) +
  geom_sf(data = airbnb_data_log_corrected, 
          aes(color = log(price)),
          # color = "#EF8A17",
          stroke = FALSE,
          alpha = 0.9,
          size = 0.4) +
  scale_color_distiller(na.value = "grey40",
                       palette = "Oranges",
                       direction = 1) +
  guides(fill = guide_colorbar(title = "Median Rent",
                               label.position = "left",
                               draw.ulim = TRUE,
                               draw.llim = TRUE,
                               title.vjust = 4,
                               override.aes = list(alpha = 0.2)),
         color = guide_colorbar(title = "Ln(Price)",
                               label.position = "left",
                               draw.ulim = TRUE,
                               draw.llim = TRUE,
                               title.vjust = 1,
                               override.aes = list(alpha = 0.2))
         ) +
  theme_void() +
  theme(text = element_text(family = "Lato"),
        legend.title = element_text(face = "bold.italic"),
        # Have to adjust margin so that 
        legend.margin = margin(0,0,0.5,0)
        # legend.spacing.y = unit(0.1, "in")
        )



# Calculate listing level and zcta level correlations between price and median rent 
listing_level_correlation_price_rent = airbnb_data_log_corrected %>% 
  # this is price of listing, and median rent of zcta that listing is in
  select(price, 
         median_rent_e) %>% 
  st_drop_geometry() %>% 
  {cor(.$price, .$median_rent_e, use = "complete.obs")} %>% 
  round(2)

zcta_level_correlation_price_rent = airbnb_avg_price_by_zcta %>% 
  # this is avg price of listing in zcta, and median rent of zcta
  select(avg_price, 
         zcta) %>% 
  st_drop_geometry() %>% 
  left_join(acs_zcta_polygons %>% 
              st_drop_geometry() %>% 
              select(zcta, median_rent_e), 
            by = "zcta") %>% 
  {cor(.$avg_price, .$median_rent_e, use = "complete.obs")} %>% 
  round(2)


```



Map 3 shows the log price of Airbnb listings (orange points) overlaid on top of a chorpoleth map of Median rent. Darker orange points are more expensive listings, and darker blue neighborhoods are higher median rent neighborhoods. I have made a couple of decisions in presenting the data that are justified below:

1) I have set increased transparency of the Median Rent choropleth map so that we can reduce visual clutter and more easily see how the listings line up with Median rent. 

3) I have turned the map title into a legend by making use of colors. This helps prime the reader for the two different variables that they will be seeing on the map. I still include two actual continuous color ramp legends to represent the scales that the colors can go to. 

2) There are 4 listings which have a list price of \$0 a night. I manually increment these values to be \$1 a night so that when we take  `log(price)`, the result will be well defined and not be -infinity. 

Generally we see that dark orange points tend to be around, and close to dark blue neighborhoods, but not necessarily in them. In other words, expensive Airbnbs tend to be near expensive neighborhoodsm which makes sense as some of these expensive neighborhoods may be tourist destinations and housing may be more scarce. The ZCTA level correlation between average listing price and median rent for the 167 ZCTAs in our study area is `r zcta_level_correlation_price_rent` and therefore backs up our understanding of a weakly positive relationship between median ZCTA rent and Airbnb price.  


# Part 2: 

## Potential Raster Datasets
Some potential raster datasets we could use to improve to this analysis would be:

1) Air quality data that measures particulate matter or ozone at various grid sizes, like [these data](https://ww2.arb.ca.gov/geographical-information-system-gis-library) from the California Air Resources Board. We could overlay this on top of our airbnb data to see if more expensive listings are correlated with being in good air quality areas, or see if good air quality is associated with more Airbnb listings. 

2) Vegetation data/indexes, that measure tree canopy cover and greenspaces at various grid sizes, like [this raster data](https://gis.data.ca.gov/datasets/CDFW::naip-2020-ndvi-california/about) from the California Department of Fish and Wildlife that provides NVDI imagery at 60 cm resolution. We could use this to look at relationships between the amount of Airbnbs/Airbnb prices and the amount of greenspace and vegetation available in the neighborhood and surrounding neighborhoods.

## Analysis Methods Explained
A heatmap of OSM sourced Bike rental locations are provided below


### Map 4: <span style="color:#CA1551">Bike Rental</span> locations from Open Street Map


```{r}

# ---- Crop bike rental data to study area -----
bike_rental_points = bike_rental_points %>% 
  st_intersection(acs_zcta_polygons) 

bike_rental_points_joined = bike_rental_points %>% 
  select(zcta, in_study_area, osm_id, name, addr.city, amenity, bicycle_rental, brand, capacity, shop, num_listings, avg_price) 



# Make heatmap of Bike rental stations
ggplot() +
  geom_sf(data = acs_zcta_polygons %>% filter(!in_study_area),
          color = "white",
          fill = "grey90"
          ) +
  geom_sf(data = acs_zcta_polygons %>% filter(in_study_area),
          fill = "grey60",
          color = "white",
          alpha = 0.65
          ) +

  geom_sf(data = bike_rental_points_joined %>% filter(in_study_area), 
          color = "#CA1551",
          stroke = FALSE,
          alpha = 0.7,
          size = 1) +
  theme_void()


```

```{r creating_buffers}

```


# Spatial Analysis

# Conclusion


# Appendix

## Appendix A: Relationship between Zip Code and ZCTA boundraies

The below interactive shows the zip code (left side) and ZCTA (right side) boundaries are slightly different. ZCTA are


```{r}
#| cache = TRUE


# zip_codes_cropped = zip_codes %>% 
#   st_crop(study_area_box)
# 
# acs_data_raw_cropped = acs_data_raw %>% 
#   st_transform("EPSG:7131") %>% 
#   st_crop(study_area_box)
# 
# mapview(zip_codes_cropped, color = "blue") | mapview(acs_data_raw_cropped, color = "blue")

```

